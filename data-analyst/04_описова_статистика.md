# Описова статистика

## Зміст розділу

-   [Що таке описова статистика](#що-таке-описова-статистика)
-   [Міри центральної тенденції](#міри-центральної-тенденції)
-   [Міри мінливості (варіації)](#міри-мінливості-варіації)
-   [Міри форми розподілу](#міри-форми-розподілу)
-   [Аналіз категоріальних даних](#аналіз-категоріальних-даних)
-   [Візуалізація описових статистик](#візуалізація-описових-статистик)
-   [Практичне застосування в аналізі даних](#практичне-застосування-в-аналізі-даних)

## Що таке описова статистика

Описова статистика (Descriptive Statistics) — це набір методів для опису, узагальнення та представлення основних характеристик набору даних. На відміну від статистики висновків (Inferential Statistics), яка спрямована на формування висновків про популяцію на основі вибірки, описова статистика зосереджується на самих даних, які ми маємо.

Основні цілі описової статистики:

-   Надання узагальненої інформації про дані в зрозумілій та компактній формі
-   Виявлення основних характеристик та закономірностей
-   Підготовка даних для подальшого аналізу та моделювання
-   Виявлення аномалій, викидів та незвичних спостережень

Описова статистика є фундаментальним інструментом дата-аналітика, оскільки вона дозволяє отримати початкове розуміння даних перед застосуванням складніших методів аналізу.

### Типи даних та їх вплив на вибір статистичних методів

Вибір конкретних методів описової статистики залежить від типу даних, з якими ми працюємо:

1. **Кількісні (числові) дані**:

    - **Неперервні**: Можуть приймати будь-яке значення в певному діапазоні (наприклад, зріст, вага, час)
    - **Дискретні**: Приймають лише певні значення, зазвичай цілі числа (наприклад, кількість дітей, кількість продажів)

2. **Якісні (категоріальні) дані**:
    - **Номінальні**: Категорії без природного порядку (наприклад, стать, колір очей, країна)
    - **Порядкові**: Категорії з природним порядком (наприклад, рівень освіти, рівень задоволеності)

| Тип даних  | Приклади                    | Допустимі операції   | Приклади описових статистик               |
| ---------- | --------------------------- | -------------------- | ----------------------------------------- |
| Номінальні | Стать, країна, кольори      | Дорівнює/не дорівнює | Мода, частота, відсотки                   |
| Порядкові  | Рівень освіти, оцінки       | Порівняння (>, <, =) | Медіана, квартилі, мода                   |
| Дискретні  | Кількість дітей, підрахунки | Арифметичні операції | Середнє, стандартне відхилення            |
| Неперервні | Зріст, вага, температура    | Арифметичні операції | Середнє, стандартне відхилення, кореляція |

### Базова термінологія

Перед тим, як заглибитися в конкретні методи, важливо ознайомитися з основною термінологією:

-   **Популяція (Population)**: Вся сукупність об'єктів, що цікавлять дослідника
-   **Вибірка (Sample)**: Підмножина популяції, яку ми вивчаємо
-   **Змінна (Variable)**: Характеристика або властивість, яку ми вимірюємо
-   **Спостереження (Observation)**: Окремий об'єкт або запис у наборі даних
-   **Розподіл (Distribution)**: Описує частоту або ймовірність різних значень змінної
-   **Параметр (Parameter)**: Характеристика популяції (наприклад, μ для середнього)
-   **Статистика (Statistic)**: Характеристика вибірки (наприклад, x̄ для вибіркового середнього)

## Міри центральної тенденції

Міри центральної тенденції (Measures of Central Tendency) — це статистичні методи, які описують "типове" або "центральне" значення набору даних. Вони дають нам уявлення про те, навколо якого значення групуються наші дані.

### Середнє арифметичне (Mean)

Середнє арифметичне — це сума всіх значень, поділена на їх кількість. Це найпоширеніша міра центральної тенденції.

Математично, для набору даних X = {x₁, x₂, ..., xₙ}, середнє арифметичне (позначається x̄) визначається як:

$$\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i = \frac{x_1 + x_2 + ... + x_n}{n}$$

#### Переваги середнього арифметичного:

-   Враховує всі значення в наборі даних
-   Має чітке математичне визначення
-   Легко інтерпретується
-   Використовується в багатьох статистичних методах

#### Недоліки середнього арифметичного:

-   Чутливе до викидів (екстремальних значень)
-   Може не представляти "типове" значення для асиметричних розподілів

#### Приклад обчислення в Python:

```python
import numpy as np
import pandas as pd

# Створення набору даних
data = [15, 21, 24, 25, 26, 28, 30, 35, 42]

# Обчислення середнього
mean_value = np.mean(data)
print(f"Середнє арифметичне: {mean_value}")  # 27.33...

# За допомогою pandas
df = pd.DataFrame({'значення': data})
print(f"Середнє з pandas: {df['значення'].mean()}")
```

### Медіана (Median)

Медіана — це значення, яке розділяє впорядкований набір даних на дві рівні половини. Якщо кількість елементів непарна, медіана — це середнє значення. Якщо парна — середнє двох центральних значень.

#### Переваги медіани:

-   Не чутлива до викидів (робастна)
-   Краще представляє "типове" значення для асиметричних розподілів
-   Добре працює з порядковими даними

#### Недоліки медіани:

-   Не враховує всі значення в наборі даних
-   Менш зручна для подальших математичних операцій

#### Приклад обчислення в Python:

```python
# Той самий набір даних
data = [15, 21, 24, 25, 26, 28, 30, 35, 42]

# Обчислення медіани
median_value = np.median(data)
print(f"Медіана: {median_value}")  # 26.0

# За допомогою pandas
print(f"Медіана з pandas: {df['значення'].median()}")
```

### Мода (Mode)

Мода — це значення, яке зустрічається найчастіше в наборі даних. Набір даних може мати одну моду (унімодальний), дві моди (бімодальний) або більше (мультимодальний).

#### Переваги моди:

-   Може використовуватися для всіх типів даних, включаючи категоріальні
-   Показує найпоширеніше значення
-   Не залежить від викидів

#### Недоліки моди:

-   Може бути нестабільною для невеликих наборів даних
-   Може не існувати (якщо всі значення зустрічаються однаково часто)
-   Може бути неоднозначною (якщо є кілька мод)

#### Приклад обчислення в Python:

```python
from scipy import stats

# Набір даних з повторюваними значеннями
data_with_mode = [15, 21, 21, 24, 25, 26, 28, 30, 30, 30, 35, 42]

# Обчислення моди
mode_result = stats.mode(data_with_mode)
print(f"Мода: {mode_result[0][0]}, Частота: {mode_result[1][0]}")  # 30, 3

# За допомогою pandas
df_mode = pd.DataFrame({'значення': data_with_mode})
print(f"Мода з pandas: {df_mode['значення'].mode().values}")
```

### Порівняння мір центральної тенденції

Для симетричних розподілів (наприклад, нормального розподілу) середнє, медіана і мода зазвичай близькі або рівні. Однак для асиметричних розподілів вони можуть значно відрізнятися.

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Створення асиметричного розподілу (позитивно скошеного)
skewed_data = [10, 12, 14, 15, 16, 18, 20, 25, 40, 60, 90]

# Обчислення мір центральної тенденції
mean_skewed = np.mean(skewed_data)
median_skewed = np.median(skewed_data)
mode_skewed = stats.mode(skewed_data)[0][0]

# Візуалізація
plt.figure(figsize=(10, 6))
sns.histplot(skewed_data, kde=True)
plt.axvline(mean_skewed, color='red', linestyle='--', label=f'Середнє: {mean_skewed:.1f}')
plt.axvline(median_skewed, color='green', linestyle='-.', label=f'Медіана: {median_skewed:.1f}')
plt.axvline(mode_skewed, color='blue', linestyle=':', label=f'Мода: {mode_skewed:.1f}')
plt.legend()
plt.title('Порівняння мір центральної тенденції для асиметричного розподілу')
plt.show()
```

### Зважене середнє (Weighted Mean)

У деяких випадках не всі спостереження мають однакову важливість. Зважене середнє враховує різну вагу (важливість) різних спостережень.

Формула зваженого середнього:

$$\bar{x}_w = \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}$$

де $w_i$ — вага i-го спостереження.

#### Приклад обчислення в Python:

```python
# Оцінки студентів та їхні ваги (кредити курсів)
grades = [85, 90, 78, 92, 88]
credits = [3, 4, 2, 4, 3]

# Обчислення зваженого середнього
weighted_mean = np.average(grades, weights=credits)
print(f"Зважене середнє: {weighted_mean}")  # 87.81...

# За допомогою pandas
df_grades = pd.DataFrame({'оцінка': grades, 'кредити': credits})
weighted_mean_pd = np.average(df_grades['оцінка'], weights=df_grades['кредити'])
print(f"Зважене середнє з pandas: {weighted_mean_pd}")
```

### Геометричне середнє (Geometric Mean)

Геометричне середнє — це n-й корінь добутку n чисел. Воно корисне для наборів даних з мультиплікативними відносинами, таких як темпи зростання або прибутковість інвестицій.

Формула:

$$G = \sqrt[n]{x_1 \times x_2 \times ... \times x_n} = \left( \prod_{i=1}^{n} x_i \right)^{1/n}$$

#### Приклад обчислення в Python:

```python
# Річні темпи зростання інвестицій
growth_rates = [1.05, 1.12, 1.08, 1.15, 1.10]  # Наприклад, 1.05 означає зростання на 5%

# Обчислення геометричного середнього
geometric_mean = stats.gmean(growth_rates)
print(f"Геометричне середнє: {geometric_mean}")  # 1.099...
print(f"Середньорічне зростання: {(geometric_mean - 1) * 100:.2f}%")  # 9.96%
```

### Гармонічне середнє (Harmonic Mean)

Гармонічне середнє — це обернене значення до середнього арифметичного обернених значень. Воно корисне для усереднення швидкостей, ставок або співвідношень.

Формула:

$$H = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + ... + \frac{1}{x_n}} = \frac{n}{\sum_{i=1}^{n} \frac{1}{x_i}}$$

#### Приклад обчислення в Python:

```python
# Швидкості на різних ділянках дороги (км/год)
speeds = [60, 80, 70, 90, 75]

# Обчислення гармонічного середнього
harmonic_mean = stats.hmean(speeds)
print(f"Гармонічне середнє швидкостей: {harmonic_mean:.2f} км/год")  # 73.49 км/год
```

## Міри мінливості (варіації)

Міри мінливості (Measures of Variability) описують, наскільки розсіяні дані навколо центрального значення. Вони дають нам уявлення про гетерогенність або гомогенність набору даних.

### Діапазон (Range)

Діапазон — це різниця між найбільшим і найменшим значеннями в наборі даних.

$$\text{Range} = \max(X) - \min(X)$$

#### Переваги діапазону:

-   Простий для обчислення та розуміння
-   Дає швидке уявлення про розмах даних

#### Недоліки діапазону:

-   Враховує лише два крайніх значення
-   Дуже чутливий до викидів

#### Приклад обчислення в Python:

```python
data = [15, 21, 24, 25, 26, 28, 30, 35, 42]

# Обчислення діапазону
data_range = np.max(data) - np.min(data)
print(f"Діапазон: {data_range}")  # 27
```

### Квартилі та міжквартильний діапазон

Квартилі розділяють упорядкований набір даних на чотири рівні частини:

-   Перший квартиль (Q1): 25-й перцентиль
-   Другий квартиль (Q2): 50-й перцентиль (медіана)
-   Третій квартиль (Q3): 75-й перцентиль

Міжквартильний діапазон (IQR) — це різниця між третім і першим квартилями:

$$\text{IQR} = Q3 - Q1$$

#### Переваги IQR:

-   Робастний до викидів
-   Показує мінливість центральних 50% даних
-   Використовується для виявлення викидів

#### Приклад обчислення в Python:

```python
# Обчислення квартилів
q1 = np.percentile(data, 25)
q3 = np.percentile(data, 75)
iqr = q3 - q1

print(f"Перший квартиль (Q1): {q1}")  # 23.0
print(f"Третій квартиль (Q3): {q3}")  # 32.5
print(f"Міжквартильний діапазон (IQR): {iqr}")  # 9.5

# За допомогою pandas
quartiles = df['значення'].quantile([0.25, 0.5, 0.75])
print("Квартилі з pandas:")
print(quartiles)
```

### Дисперсія (Variance)

Дисперсія вимірює, наскільки значення в наборі даних відрізняються від середнього арифметичного. Це середній квадрат відхилень від середнього.

Для вибірки дисперсія (позначається s²) обчислюється за формулою:

$$s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$

#### Переваги дисперсії:

-   Враховує всі значення в наборі даних
-   Є основою для багатьох статистичних методів
-   Має корисні математичні властивості

#### Недоліки дисперсії:

-   Одиниці вимірювання — квадрати оригінальних одиниць
-   Чутлива до викидів
-   Може бути складною для інтерпретації

#### Приклад обчислення в Python:

```python
# Обчислення дисперсії
variance = np.var(data, ddof=1)  # ddof=1 для вибіркової дисперсії
print(f"Дисперсія: {variance}")  # 62.75

# За допомогою pandas
print(f"Дисперсія з pandas: {df['значення'].var()}")
```

### Стандартне відхилення (Standard Deviation)

Стандартне відхилення — це квадратний корінь з дисперсії. Воно має ті самі одиниці вимірювання, що й оригінальні дані, що полегшує інтерпретацію.

$$s = \sqrt{s^2} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2}$$

#### Переваги стандартного відхилення:

-   Має ті самі одиниці вимірювання, що й оригінальні дані
-   Широко використовується в статистиці
-   Корисне для визначення, наскільки значення відхиляються від середнього

#### Приклад обчислення в Python:

```python
# Обчислення стандартного відхилення
std_dev = np.std(data, ddof=1)
print(f"Стандартне відхилення: {std_dev}")  # 7.92...

# За допомогою pandas
print(f"Стандартне відхилення з pandas: {df['значення'].std()}")
```

### Коефіцієнт варіації (Coefficient of Variation)

Коефіцієнт варіації — це відносна міра дисперсії, яка виражає стандартне відхилення як відсоток від середнього. Він корисний для порівняння мінливості наборів даних з різними одиницями вимірювання або середніми значеннями.

$$CV = \frac{s}{\bar{x}} \times 100\%$$

#### Приклад обчислення в Python:

```python
# Обчислення коефіцієнта варіації
cv = (std_dev / mean_value) * 100
print(f"Коефіцієнт варіації: {cv:.2f}%")  # 28.98%
```

### Середнє абсолютне відхилення (Mean Absolute Deviation)

Середнє абсолютне відхилення (MAD) — це середнє абсолютних відхилень від середнього арифметичного.

$$\text{MAD} = \frac{1}{n} \sum_{i=1}^{n} |x_i - \bar{x}|$$

#### Переваги MAD:

-   Має ті самі одиниці вимірювання, що й оригінальні дані
-   Простіше для інтерпретації, ніж дисперсія
-   Менш чутливе до викидів, ніж дисперсія

#### Приклад обчислення в Python:

```python
# Обчислення середнього абсолютного відхилення
mad = np.mean(np.abs(data - mean_value))
print(f"Середнє абсолютне відхилення: {mad}")  # 6.22...

# За допомогою pandas
print(f"MAD з pandas: {df['значення'].mad()}")
```

## Міри форми розподілу

Міри форми розподілу описують форму розподілу даних, зокрема його симетрію та пікоподібність.

### Асиметрія (Skewness)

Асиметрія вимірює симетрію розподілу. Нульова асиметрія вказує на симетричний розподіл. Позитивна асиметрія (правостороння) означає, що розподіл має довгий "хвіст" праворуч, а негативна асиметрія (лівостороння) — ліворуч.

Формула вибіркової асиметрії:

$$\text{Skewness} = \frac{n}{(n-1)(n-2)} \sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{s} \right)^3$$

#### Інтерпретація асиметрії:

-   Близько до 0: розподіл симетричний
-   > 0: позитивна асиметрія (довгий хвіст праворуч)
-   < 0: негативна асиметрія (довгий хвіст ліворуч)

#### Приклад обчислення в Python:

```python
from scipy import stats

# Обчислення асиметрії
skewness = stats.skew(data)
print(f"Асиметрія: {skewness}")  # 0.41...

# За допомогою pandas
print(f"Асиметрія з pandas: {df['значення'].skew()}")

# Візуалізація різних типів асиметрії
plt.figure(figsize=(15, 5))

# Негативна асиметрія
plt.subplot(1, 3, 1)
negative_skew = np.random.beta(5, 2, 1000)
sns.histplot(negative_skew, kde=True)
plt.title(f'Негативна асиметрія: {stats.skew(negative_skew):.2f}')

# Симетричний розподіл
plt.subplot(1, 3, 2)
symmetric = np.random.normal(0, 1, 1000)
sns.histplot(symmetric, kde=True)
plt.title(f'Симетричний розподіл: {stats.skew(symmetric):.2f}')

# Позитивна асиметрія
plt.subplot(1, 3, 3)
positive_skew = np.random.beta(2, 5, 1000)
sns.histplot(positive_skew, kde=True)
plt.title(f'Позитивна асиметрія: {stats.skew(positive_skew):.2f}')

plt.tight_layout()
plt.show()
```

### Ексцес (Kurtosis)

Ексцес вимірює "пікоподібність" розподілу — наскільки розподіл концентрується навколо піку порівняно з хвостами.

Формула вибіркового ексцесу (надлишковий ексцес):

$$\text{Kurtosis} = \frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{s} \right)^4 - \frac{3(n-1)^2}{(n-2)(n-3)}$$

#### Інтерпретація ексцесу:

-   = 0: мезокуртичний розподіл (як нормальний)
-   > 0: лептокуртичний розподіл (більш гострий пік, важчі хвости)
-   < 0: платикуртичний розподіл (більш плоский, легші хвости)

#### Приклад обчислення в Python:

```python
# Обчислення ексцесу
kurtosis = stats.kurtosis(data)
print(f"Ексцес: {kurtosis}")  # -0.50...

# За допомогою pandas
print(f"Ексцес з pandas: {df['значення'].kurtosis()}")

# Візуалізація різних типів ексцесу
plt.figure(figsize=(15, 5))

# Платикуртичний (від'ємний ексцес)
plt.subplot(1, 3, 1)
platykurtic = np.random.beta(3, 3, 1000)
sns.histplot(platykurtic, kde=True)
plt.title(f'Платикуртичний: {stats.kurtosis(platykurtic):.2f}')

# Мезокуртичний (нормальний розподіл)
plt.subplot(1, 3, 2)
mesokurtic = np.random.normal(0, 1, 1000)
sns.histplot(mesokurtic, kde=True)
plt.title(f'Мезокуртичний: {stats.kurtosis(mesokurtic):.2f}')

# Лептокуртичний (додатний ексцес)
plt.subplot(1, 3, 3)
leptokurtic = np.random.laplace(0, 1, 1000)
sns.histplot(leptokurtic, kde=True)
plt.title(f'Лептокуртичний: {stats.kurtosis(leptokurtic):.2f}')

plt.tight_layout()
plt.show()
```

## Аналіз категоріальних даних

Для категоріальних даних використовуються специфічні методи описової статистики, оскільки багато стандартних статистичних мір не застосовуються до нечислових даних.

### Частотні розподіли

Частотний розподіл показує, скільки разів кожна категорія зустрічається в наборі даних.

#### Приклад обчислення в Python:

```python
# Категоріальні дані
categories = ['A', 'B', 'A', 'C', 'B', 'B', 'A', 'D', 'B', 'C', 'A', 'B']

# Обчислення частот
frequency = {}
for category in categories:
    if category in frequency:
        frequency[category] += 1
    else:
        frequency[category] = 1

print("Частотний розподіл:")
for category, count in frequency.items():
    print(f"{category}: {count}")

# За допомогою pandas
df_cat = pd.DataFrame({'категорія': categories})
print("\nЧастотний розподіл з pandas:")
print(df_cat['категорія'].value_counts())

# Відносні частоти
print("\nВідносні частоти:")
print(df_cat['категорія'].value_counts(normalize=True))
```

### Візуалізація категоріальних даних

Для візуалізації категоріальних даних часто використовуються стовпчасті діаграми, кругові діаграми та таблиці спряженості.

```python
# Стовпчаста діаграма
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.countplot(x='категорія', data=df_cat, order=df_cat['категорія'].value_counts().index)
plt.title('Стовпчаста діаграма частот')

# Кругова діаграма
plt.subplot(1, 2, 2)
df_cat['категорія'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)
plt.title('Кругова діаграма категорій')
plt.ylabel('')

plt.tight_layout()
plt.show()
```

### Таблиці спряженості (Contingency Tables)

Таблиці спряженості показують взаємозв'язок між двома категоріальними змінними.

```python
# Дві категоріальні змінні
df_contingency = pd.DataFrame({
    'стать': ['Ч', 'Ж', 'Ч', 'Ч', 'Ж', 'Ж', 'Ч', 'Ж', 'Ч', 'Ж', 'Ч', 'Ж'],
    'освіта': ['Вища', 'Середня', 'Вища', 'Середня', 'Вища', 'Вища',
               'Середня', 'Вища', 'Вища', 'Середня', 'Середня', 'Вища']
})

# Створення таблиці спряженості
contingency_table = pd.crosstab(df_contingency['стать'], df_contingency['освіта'])
print("Таблиця спряженості:")
print(contingency_table)

# Відносні частоти (за рядками)
print("\nВідносні частоти за рядками:")
print(pd.crosstab(df_contingency['стать'], df_contingency['освіта'], normalize='index'))

# Візуалізація таблиці спряженості
plt.figure(figsize=(10, 6))
sns.heatmap(contingency_table, annot=True, fmt='d', cmap='Blues')
plt.title('Візуалізація таблиці спряженості')
plt.show()
```

### Міри асоціації

Для вимірювання зв'язку між категоріальними змінними використовуються спеціальні міри асоціації, такі як хі-квадрат, коефіцієнт Крамера V та коефіцієнт контингенції.

```python
from scipy.stats import chi2_contingency

# Хі-квадрат тест
chi2, p, dof, expected = chi2_contingency(contingency_table)
print(f"Хі-квадрат: {chi2:.4f}")
print(f"p-значення: {p:.4f}")

# Коефіцієнт Крамера V
n = contingency_table.sum().sum()
phi2 = chi2 / n
r, k = contingency_table.shape
phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))
rcorr = r - ((r-1)**2)/(n-1)
kcorr = k - ((k-1)**2)/(n-1)
cramer_v = np.sqrt(phi2corr / min(kcorr-1, rcorr-1))
print(f"Коефіцієнт Крамера V: {cramer_v:.4f}")
```

## Візуалізація описових статистик

Візуалізація є потужним інструментом для представлення та розуміння описових статистик. Різні типи графіків підходять для різних типів даних та різних аспектів аналізу.

### Гістограми

Гістограми показують розподіл числових даних, розділяючи діапазон значень на інтервали (біни) і підраховуючи, скільки значень потрапляє в кожен інтервал.

```python
# Створення набору даних
np.random.seed(42)
normal_data = np.random.normal(loc=100, scale=15, size=1000)

# Гістограма
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(normal_data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(np.mean(normal_data), color='red', linestyle='--',
            label=f'Середнє: {np.mean(normal_data):.1f}')
plt.axvline(np.median(normal_data), color='green', linestyle='-.',
            label=f'Медіана: {np.median(normal_data):.1f}')
plt.title('Гістограма нормального розподілу')
plt.xlabel('Значення')
plt.ylabel('Частота')
plt.legend()

# Гістограма з ядерною оцінкою густини (KDE)
plt.subplot(1, 2, 2)
sns.histplot(normal_data, bins=30, kde=True, color='skyblue')
plt.title('Гістограма з KDE')
plt.xlabel('Значення')
plt.ylabel('Частота')

plt.tight_layout()
plt.show()
```

### Коробкові діаграми (Box Plots)

Коробкові діаграми показують розподіл даних на основі п'яти статистик: мінімум, перший квартиль (Q1), медіана, третій квартиль (Q3) та максимум. Вони також виділяють викиди.

```python
# Створення набору даних з викидами
data_with_outliers = np.concatenate([normal_data, [150, 160, 170, 30, 40]])

# Коробкова діаграма
plt.figure(figsize=(10, 6))
sns.boxplot(x=data_with_outliers)
plt.title('Коробкова діаграма з викидами')
plt.xlabel('Значення')

plt.show()

# Порівняльні коробкові діаграми
groups = ['A'] * 200 + ['B'] * 150 + ['C'] * 100
values = np.concatenate([
    np.random.normal(loc=80, scale=10, size=200),  # Група A
    np.random.normal(loc=90, scale=15, size=150),  # Група B
    np.random.normal(loc=70, scale=8, size=100)    # Група C
])

df_groups = pd.DataFrame({'група': groups, 'значення': values})

plt.figure(figsize=(10, 6))
sns.boxplot(x='група', y='значення', data=df_groups)
plt.title('Порівняння розподілів за групами')
plt.xlabel('Група')
plt.ylabel('Значення')

plt.show()
```

### Діаграми розсіювання (Scatter Plots)

Діаграми розсіювання показують взаємозв'язок між двома числовими змінними.

```python
# Створення двовимірних даних з кореляцією
np.random.seed(42)
x = np.random.normal(0, 1, 100)
y = x * 0.8 + np.random.normal(0, 0.5, 100)  # Кореляція приблизно 0.8

# Діаграма розсіювання
plt.figure(figsize=(10, 6))
plt.scatter(x, y, alpha=0.7)
plt.title('Діаграма розсіювання')
plt.xlabel('X')
plt.ylabel('Y')
plt.grid(True, linestyle='--', alpha=0.7)

# Додавання лінії регресії
slope, intercept = np.polyfit(x, y, 1)
plt.plot(x, slope*x + intercept, color='red', linestyle='--',
         label=f'y = {slope:.2f}x + {intercept:.2f}')

# Додавання коефіцієнта кореляції
correlation = np.corrcoef(x, y)[0, 1]
plt.text(0.05, 0.95, f'Кореляція: {correlation:.2f}', transform=plt.gca().transAxes,
         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))

plt.legend()
plt.show()
```

### Теплові карти (Heatmaps)

Теплові карти корисні для візуалізації матриць даних, таких як кореляційні матриці.

```python
# Створення багатовимірних даних
np.random.seed(42)
n = 100
data_multi = pd.DataFrame({
    'змінна_1': np.random.normal(0, 1, n),
    'змінна_2': np.random.normal(0, 1, n),
    'змінна_3': np.random.normal(0, 1, n),
    'змінна_4': np.random.normal(0, 1, n),
    'змінна_5': np.random.normal(0, 1, n)
})

# Додавання кореляції між змінними
data_multi['змінна_2'] = data_multi['змінна_1'] * 0.7 + np.random.normal(0, 0.7, n)
data_multi['змінна_4'] = data_multi['змінна_3'] * 0.8 + np.random.normal(0, 0.6, n)
data_multi['змінна_5'] = data_multi['змінна_4'] * 0.6 + np.random.normal(0, 0.8, n)

# Обчислення кореляційної матриці
corr_matrix = data_multi.corr()

# Візуалізація кореляційної матриці
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0,
            linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('Кореляційна матриця')
plt.tight_layout()
plt.show()
```

### Діаграми щільності (Density Plots)

Діаграми щільності показують оцінку густини ймовірності неперервних даних.

```python
# Створення різних розподілів
np.random.seed(42)
normal = np.random.normal(0, 1, 1000)
bimodal = np.concatenate([np.random.normal(-2, 0.5, 500), np.random.normal(2, 0.5, 500)])
skewed = np.random.exponential(1, 1000)

# Візуалізація розподілів
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.kdeplot(normal, fill=True)
plt.title('Нормальний розподіл')
plt.xlabel('Значення')
plt.ylabel('Густина')

plt.subplot(1, 3, 2)
sns.kdeplot(bimodal, fill=True)
plt.title('Бімодальний розподіл')
plt.xlabel('Значення')
plt.ylabel('Густина')

plt.subplot(1, 3, 3)
sns.kdeplot(skewed, fill=True)
plt.title('Скошений розподіл')
plt.xlabel('Значення')
plt.ylabel('Густина')

plt.tight_layout()
plt.show()
```

## Практичне застосування в аналізі даних

Описова статистика є основою для багатьох типів аналізу даних і має широке практичне застосування.

### Підготовка та розвідувальний аналіз даних

Описова статистика є першим кроком у розвідувальному аналізі даних (EDA), допомагаючи зрозуміти основні характеристики даних перед застосуванням складніших методів.

```python
# Завантаження даних
# Використовуємо набір даних про пасажирів Титаніка
import seaborn as sns
titanic = sns.load_dataset('titanic')

# Базова описова статистика
print("Опис числових змінних:")
print(titanic.describe())

print("\nОпис категоріальних змінних:")
print(titanic.describe(include=['object']))

# Частоти категоріальних змінних
print("\nРозподіл за класом:")
print(titanic['class'].value_counts())

print("\nРозподіл за статтю:")
print(titanic['sex'].value_counts())

print("\nРозподіл за виживанням:")
print(titanic['survived'].value_counts())
```

### Аналіз зв'язків між змінними

Описова статистика допомагає виявити та кількісно оцінити зв'язки між змінними.

```python
# Перевірка зв'язку між виживанням та класом
survival_by_class = pd.crosstab(titanic['survived'], titanic['class'], normalize='index')
print("Відсоток виживання за класом:")
print(survival_by_class * 100)

# Візуалізація
plt.figure(figsize=(10, 6))
sns.countplot(x='class', hue='survived', data=titanic)
plt.title('Виживання за класом')
plt.xlabel('Клас')
plt.ylabel('Кількість пасажирів')
plt.legend(title='Вижив', labels=['Ні', 'Так'])
plt.show()

# Кореляція між числовими змінними
numeric_vars = titanic[['survived', 'age', 'fare']]
correlation = numeric_vars.corr()
print("\nКореляційна матриця:")
print(correlation)

# Візуалізація кореляції
plt.figure(figsize=(8, 6))
sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)
plt.title('Кореляція між виживанням, віком та вартістю квитка')
plt.show()
```

### Виявлення викидів та аномалій

Описова статистика допомагає виявити викиди та аномалії в даних.

```python
# Перевірка розподілу вартості квитка
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.boxplot(x='fare', data=titanic)
plt.title('Коробкова діаграма вартості квитка')
plt.xlabel('Вартість квитка')

plt.subplot(1, 2, 2)
sns.histplot(titanic['fare'], bins=30, kde=True)
plt.title('Розподіл вартості квитка')
plt.xlabel('Вартість квитка')
plt.ylabel('Кількість пасажирів')

plt.tight_layout()
plt.show()

# Виявлення викидів за правилом 1.5*IQR
Q1 = titanic['fare'].quantile(0.25)
Q3 = titanic['fare'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = titanic[(titanic['fare'] < lower_bound) | (titanic['fare'] > upper_bound)]
print(f"Кількість викидів у вартості квитка: {len(outliers)}")
print("Приклади викидів:")
print(outliers[['name', 'fare', 'class']].head())
```

### Порівняння груп та когорт

Описова статистика дозволяє порівнювати різні групи та когорти.

```python
# Порівняння віку пасажирів за статтю та класом
plt.figure(figsize=(12, 6))
sns.boxplot(x='class', y='age', hue='sex', data=titanic)
plt.title('Розподіл віку за класом та статтю')
plt.xlabel('Клас')
plt.ylabel('Вік')
plt.legend(title='Стать')
plt.show()

# Статистичні підсумки для кожної групи
age_summary = titanic.groupby(['class', 'sex'])['age'].agg(['count', 'mean', 'std', 'min', 'median', 'max'])
print("Статистика віку за класом та статтю:")
print(age_summary)
```

### Відстеження змін та тенденцій у часі

Описова статистика допомагає відстежувати зміни та тенденції протягом часу.

```python
# Створення часових даних
np.random.seed(42)
dates = pd.date_range(start='2020-01-01', end='2022-12-31', freq='M')
values = np.cumsum(np.random.normal(0, 1, len(dates))) + 100

time_series = pd.DataFrame({'дата': dates, 'значення': values})

# Зведена статистика за роками
yearly_stats = time_series.set_index('дата').resample('Y').agg(['mean', 'std', 'min', 'max'])
print("Статистика за роками:")
print(yearly_stats)

# Візуалізація часового ряду
plt.figure(figsize=(12, 6))
plt.plot(time_series['дата'], time_series['значення'], marker='o')
plt.title('Зміни значень з часом')
plt.xlabel('Дата')
plt.ylabel('Значення')
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```

### Комплексний аналіз з використанням описової статистики

Комбінування різних методів описової статистики дозволяє отримати комплексне розуміння даних.

```python
# Комплексний аналіз даних про Титанік

# 1. Загальні статистики
print("Загальна кількість пасажирів:", len(titanic))
print("Кількість виживших:", titanic['survived'].sum())
print("Відсоток виживання: {:.1f}%".format(titanic['survived'].mean() * 100))

# 2. Розподіл пасажирів за класом, статтю та віковими групами
titanic['age_group'] = pd.cut(titanic['age'], bins=[0, 18, 35, 50, 100],
                             labels=['0-18', '19-35', '36-50', '51+'])

passenger_distribution = titanic.groupby(['class', 'sex', 'age_group']).size().unstack(fill_value=0)
print("\nРозподіл пасажирів за класом, статтю та віковими групами:")
print(passenger_distribution)

# 3. Аналіз виживання за різними факторами
survival_factors = titanic.groupby(['class', 'sex', 'age_group'])['survived'].mean().unstack(fill_value=0) * 100
print("\nВідсоток виживання за класом, статтю та віковими групами:")
print(survival_factors)

# 4. Візуалізація результатів
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
sns.countplot(x='class', data=titanic)
plt.title('Розподіл пасажирів за класом')

plt.subplot(2, 2, 2)
sns.countplot(x='class', hue='survived', data=titanic)
plt.title('Виживання за класом')
plt.legend(title='Вижив', labels=['Ні', 'Так'])

plt.subplot(2, 2, 3)
sns.barplot(x='class', y='survived', data=titanic)
plt.title('Відсоток виживання за класом')
plt.ylabel('Відсоток виживання')

plt.subplot(2, 2, 4)
sns.barplot(x='class', y='survived', hue='sex', data=titanic)
plt.title('Відсоток виживання за класом та статтю')
plt.ylabel('Відсоток виживання')
plt.legend(title='Стать')

plt.tight_layout()
plt.show()
```

## Висновки

Описова статистика є незамінним інструментом в арсеналі дата-аналітика, забезпечуючи основу для розуміння та інтерпретації даних. Вона дозволяє:

1. **Узагальнювати дані** — зводити великі обсяги інформації до зрозумілих показників
2. **Виявляти закономірності** — знаходити тенденції, взаємозв'язки та патерни
3. **Оцінювати якість даних** — виявляти викиди, пропущені значення та аномалії
4. **Готувати дані для моделювання** — розуміти розподіли та зв'язки для правильного вибору методів аналізу
5. **Комунікувати результати** — представляти дані в зрозумілій для нетехнічних користувачів формі

Ключовими аспектами ефективного використання описової статистики є:

-   Вибір підходящих статистичних мір відповідно до типу даних та цілей аналізу
-   Поєднання числових мір з візуалізацією для кращого розуміння
-   Врахування обмежень кожної міри та потенційних помилок інтерпретації
-   Використання описової статистики як відправної точки для більш складного аналізу

Описова статистика є лише початком аналітичного процесу, але вона закладає основу для всіх подальших етапів, від висунення гіпотез до побудови складних моделей та прийняття рішень.

---

**Попередня тема:** [Життєвий цикл аналізу даних](./03_життєвий_цикл_аналізу_даних.md)

**Наступна тема:** [Теорія ймовірності](./05_теорія_ймовірності.md)
