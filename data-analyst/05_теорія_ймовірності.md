# Теорія ймовірності

## Зміст розділу

-   [Базові поняття теорії ймовірності](#базові-поняття-теорії-ймовірності)
-   [Правила та аксіоми теорії ймовірності](#правила-та-аксіоми-теорії-ймовірності)
-   [Умовна ймовірність](#умовна-ймовірність)
-   [Формула повної ймовірності](#формула-повної-ймовірності)
-   [Формула Байєса](#формула-байєса)
-   [Незалежні події](#незалежні-події)
-   [Випадкові величини](#випадкові-величини)
-   [Математичне сподівання](#математичне-сподівання)
-   [Дисперсія та стандартне відхилення](#дисперсія-та-стандартне-відхилення)
-   [Функції розподілу ймовірностей](#функції-розподілу-ймовірностей)
-   [Практичне застосування в аналізі даних](#практичне-застосування-в-аналізі-даних)

## Базові поняття теорії ймовірності

Теорія ймовірності — це розділ математики, що вивчає закономірності випадкових явищ. Вона є фундаментом для статистичного аналізу даних, машинного навчання та прийняття рішень в умовах невизначеності.

### Випадковий експеримент

**Випадковий експеримент** — це процес, результат якого неможливо передбачити з абсолютною впевненістю до його виконання, але можна описати всі можливі результати.

Приклади випадкових експериментів:

-   Підкидання монети
-   Кидання гральних кубиків
-   Витягування карти з колоди
-   Вимірювання часу відгуку сервера
-   Опитування випадкових респондентів

### Простір елементарних подій

**Простір елементарних подій** (позначається Ω) — це множина всіх можливих результатів випадкового експерименту.

Приклади:

-   Для підкидання монети: Ω = {орел, решка}
-   Для кидання кубика: Ω = {1, 2, 3, 4, 5, 6}
-   Для витягування карти з колоди: Ω = {всі 52 карти}

### Подія

**Подія** — це підмножина простору елементарних подій. Подія A відбувається, якщо результат експерименту належить до цієї підмножини.

Типи подій:

-   **Елементарна подія** — неподільний результат експерименту
-   **Складна подія** — підмножина з кількох елементарних подій
-   **Достовірна подія** — подія, яка завжди відбувається (збігається з Ω)
-   **Неможлива подія** — подія, яка ніколи не відбувається (позначається ∅)
-   **Протилежна подія** — подія, яка відбувається тоді і тільки тоді, коли вихідна подія не відбувається (позначається A̅ або A')

### Операції над подіями

Оскільки події є множинами, до них застосовуються операції теорії множин:

-   **Об'єднання** (A ∪ B): подія, яка відбувається, якщо відбувається хоча б одна з подій A або B
-   **Перетин** (A ∩ B): подія, яка відбувається, якщо відбуваються обидві події A і B одночасно
-   **Різниця** (A \ B): подія, яка відбувається, якщо відбувається подія A, але не відбувається подія B
-   **Доповнення** (A̅ або A'): подія, яка відбувається, якщо не відбувається подія A

### Ймовірність події

**Ймовірність події** — це числова міра, яка характеризує шанс події відбутися. Позначається P(A) для події A.

Основні властивості ймовірності:

1. 0 ≤ P(A) ≤ 1 для будь-якої події A
2. P(Ω) = 1 (ймовірність достовірної події дорівнює 1)
3. P(∅) = 0 (ймовірність неможливої події дорівнює 0)

#### Класичне визначення ймовірності

Якщо всі елементарні події рівноймовірні, то ймовірність події A дорівнює відношенню кількості сприятливих елементарних подій до загальної кількості елементарних подій:

$$P(A) = \frac{|A|}{|\Omega|} = \frac{\text{кількість сприятливих результатів}}{\text{загальна кількість можливих результатів}}$$

Приклад:
Ймовірність випадання парного числа при киданні кубика:

-   Сприятливі результати: {2, 4, 6}
-   Загальна кількість можливих результатів: {1, 2, 3, 4, 5, 6}
-   P(парне число) = 3/6 = 1/2 = 0.5

#### Статистичне визначення ймовірності

Якщо експеримент можна повторити багато разів у однакових умовах, то ймовірність події можна оцінити як:

$$P(A) \approx \frac{n_A}{n} = \frac{\text{кількість появ події A}}{\text{загальна кількість випробувань}}$$

При збільшенні кількості випробувань статистична ймовірність наближається до теоретичної.

```python
import numpy as np
import matplotlib.pyplot as plt

# Моделювання підкидання монети
np.random.seed(42)
n_trials = 10000
coin_flips = np.random.choice(['орел', 'решка'], size=n_trials)

# Обчислення відносної частоти випадання орла
heads_count = np.cumsum(coin_flips == 'орел')
proportions = heads_count / np.arange(1, n_trials + 1)

# Візуалізація
plt.figure(figsize=(10, 6))
plt.plot(range(1, n_trials + 1), proportions)
plt.axhline(y=0.5, color='r', linestyle='--', label='Теоретична ймовірність')
plt.xscale('log')
plt.xlabel('Кількість випробувань')
plt.ylabel('Відносна частота випадання орла')
plt.title('Збіжність відносної частоти до теоретичної ймовірності')
plt.grid(True)
plt.legend()
plt.show()
```

#### Геометричне визначення ймовірності

Для експериментів з неперервними результатами ймовірність події можна розглядати як відношення міри (площі, об'єму) області, що відповідає події, до міри всього простору елементарних подій:

$$P(A) = \frac{\text{міра області A}}{\text{міра всього простору Ω}}$$

Приклад:
Ймовірність того, що випадкова точка, рівномірно розподілена всередині кола радіусом R, потрапить всередину вписаного квадрата зі стороною a = R√2:

-   Площа квадрата: a² = 2R²
-   Площа кола: πR²
-   P(точка у квадраті) = 2R²/(πR²) = 2/π ≈ 0.637

## Правила та аксіоми теорії ймовірності

Теорія ймовірності базується на аксіоматичній системі, запропонованій Андрієм Колмогоровим, яка формалізує інтуїтивні уявлення про ймовірність.

### Аксіоми теорії ймовірності

1. **Невід'ємність**: P(A) ≥ 0 для будь-якої події A
2. **Нормованість**: P(Ω) = 1, де Ω — простір елементарних подій
3. **Адитивність**: Якщо події A₁, A₂, ..., Aₙ, ... попарно несумісні (тобто A*i ∩ A_j = ∅ для i ≠ j), то:
   $$P\left(\bigcup*{i=1}^{\infty} A*i\right) = \sum*{i=1}^{\infty} P(A_i)$$

### Наслідки аксіом

1. P(∅) = 0 (ймовірність неможливої події дорівнює 0)
2. P(A̅) = 1 - P(A) (ймовірність протилежної події)
3. Якщо A ⊂ B, то P(A) ≤ P(B) (монотонність)
4. 0 ≤ P(A) ≤ 1 для будь-якої події A
5. P(A ∪ B) = P(A) + P(B) - P(A ∩ B) (формула включення-виключення)

### Формула включення-виключення

Для двох подій:
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

Для трьох подій:
$$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$$

Загальна формула для n подій:
$$P\left(\bigcup_{i=1}^{n} A_i\right) = \sum_{i=1}^{n} P(A_i) - \sum_{i < j} P(A_i \cap A_j) + \sum_{i < j < k} P(A_i \cap A_j \cap A_k) - ... + (-1)^{n-1} P(A_1 \cap A_2 \cap ... \cap A_n)$$

### Приклад застосування правил

Розглянемо експеримент з витягуванням карти з колоди:

-   A: витягнута карта — туз
-   B: витягнута карта — червоної масті (черви або бубни)

У стандартній колоді 52 карти, 4 тузи (по одному в кожній масті) і 26 карт червоної масті.

Обчислимо ймовірність витягнути або туза, або червону карту:

-   P(A) = 4/52 = 1/13 (ймовірність витягнути туза)
-   P(B) = 26/52 = 1/2 (ймовірність витягнути червону карту)
-   P(A ∩ B) = 2/52 = 1/26 (ймовірність витягнути червоного туза)

Використовуючи формулу включення-виключення:
$$P(A \cup B) = P(A) + P(B) - P(A \cap B) = 1/13 + 1/2 - 1/26 = 2/26 + 13/26 - 1/26 = 14/26 = 7/13 \approx 0.538$$

```python
# Перевірка за допомогою симуляції
import numpy as np

np.random.seed(42)
n_trials = 100000

# Створення колоди карт
suits = ['♥', '♦', '♣', '♠']
ranks = ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']
deck = [(rank, suit) for suit in suits for rank in ranks]

# Симуляція витягування карт
results = []
for _ in range(n_trials):
    card = deck[np.random.randint(0, 52)]
    is_ace = card[0] == 'A'
    is_red = card[1] in ['♥', '♦']
    results.append((is_ace, is_red))

# Обчислення ймовірностей
p_ace = sum(ace for ace, _ in results) / n_trials
p_red = sum(red for _, red in results) / n_trials
p_ace_and_red = sum(ace and red for ace, red in results) / n_trials
p_ace_or_red = sum(ace or red for ace, red in results) / n_trials

print(f"P(туз) = {p_ace:.6f} ≈ 1/13 = {1/13:.6f}")
print(f"P(червона карта) = {p_red:.6f} ≈ 1/2 = {1/2:.6f}")
print(f"P(туз і червона карта) = {p_ace_and_red:.6f} ≈ 1/26 = {1/26:.6f}")
print(f"P(туз або червона карта) = {p_ace_or_red:.6f} ≈ 7/13 = {7/13:.6f}")
print(f"P(туз) + P(червона карта) - P(туз і червона карта) = {p_ace + p_red - p_ace_and_red:.6f}")
```

## Умовна ймовірність

Умовна ймовірність події A за умови, що відбулася подія B, позначається P(A|B) і визначається як:

$$P(A|B) = \frac{P(A \cap B)}{P(B)}, \text{де } P(B) > 0$$

Інтуїтивно, це ймовірність події A в новому просторі елементарних подій, який обмежений тим, що подія B вже відбулася.

### Приклад умовної ймовірності

Розглянемо експеримент з витягуванням карти з колоди:

-   A: витягнута карта — туз
-   B: витягнута карта — червоної масті

Обчислимо ймовірність витягнути туза за умови, що карта червоної масті:

-   P(A ∩ B) = 2/52 = 1/26 (ймовірність витягнути червоного туза)
-   P(B) = 26/52 = 1/2 (ймовірність витягнути червону карту)

$$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{1/26}{1/2} = \frac{1}{13} \approx 0.077$$

Це має сенс, оскільки з 26 червоних карт 2 є тузами, тому 2/26 = 1/13.

```python
# Перевірка за допомогою симуляції
red_cards = [(rank, suit) for suit in ['♥', '♦'] for rank in ranks]
red_aces = [(rank, suit) for suit in ['♥', '♦'] for rank in ranks if rank == 'A']

print(f"Кількість червоних карт: {len(red_cards)}")
print(f"Кількість червоних тузів: {len(red_aces)}")
print(f"Умовна ймовірність P(туз|червона карта) = {len(red_aces)/len(red_cards):.6f}")
```

### Властивості умовної ймовірності

1. P(A|B) ≥ 0 для будь-яких подій A і B, де P(B) > 0
2. P(Ω|B) = 1 для будь-якої події B, де P(B) > 0
3. Якщо події A₁, A₂, ..., Aₙ попарно несумісні, то:
   $$P\left(\bigcup_{i=1}^{n} A_i|B\right) = \sum_{i=1}^{n} P(A_i|B)$$

### Зв'язок з перетином подій

З визначення умовної ймовірності можна отримати формулу для ймовірності перетину подій:

$$P(A \cap B) = P(B) \cdot P(A|B) = P(A) \cdot P(B|A)$$

Ця формула часто використовується при розв'язанні задач з умовною ймовірністю.

## Формула повної ймовірності

Формула повної ймовірності дозволяє обчислити ймовірність події через умовні ймовірності.

Якщо події B₁, B₂, ..., Bₙ утворюють повну групу подій (тобто вони попарно несумісні і їх об'єднання дорівнює всьому простору елементарних подій: B₁ ∪ B₂ ∪ ... ∪ Bₙ = Ω), то для будь-якої події A:

$$P(A) = \sum_{i=1}^{n} P(B_i) \cdot P(A|B_i)$$

### Приклад застосування формули повної ймовірності

Припустимо, у нас є три ящики:

-   Ящик 1 містить 2 білі та 3 чорні кулі
-   Ящик 2 містить 4 білі та 1 чорну кулю
-   Ящик 3 містить 3 білі та 2 чорні кулі

Експеримент полягає в тому, що ми вибираємо ящик випадково (з однаковою ймовірністю), а потім витягуємо з нього одну кулю. Яка ймовірність витягнути білу кулю?

Нехай:

-   A: витягнута куля біла
-   B₁, B₂, B₃: вибрано ящик 1, 2, 3 відповідно

Тоді:

-   P(B₁) = P(B₂) = P(B₃) = 1/3 (ящики вибираються з однаковою ймовірністю)
-   P(A|B₁) = 2/5 (у першому ящику 2 білі з 5 куль)
-   P(A|B₂) = 4/5 (у другому ящику 4 білі з 5 куль)
-   P(A|B₃) = 3/5 (у третьому ящику 3 білі з 5 куль)

За формулою повної ймовірності:
$$P(A) = P(B_1) \cdot P(A|B_1) + P(B_2) \cdot P(A|B_2) + P(B_3) \cdot P(A|B_3) = \frac{1}{3} \cdot \frac{2}{5} + \frac{1}{3} \cdot \frac{4}{5} + \frac{1}{3} \cdot \frac{3}{5} = \frac{1}{3} \cdot \frac{9}{5} = \frac{3}{5} = 0.6$$

```python
# Перевірка за допомогою симуляції
np.random.seed(42)
n_trials = 100000

# Вміст ящиків (кількість білих та чорних куль)
boxes = [(2, 3), (4, 1), (3, 2)]

results = []
for _ in range(n_trials):
    # Вибір ящика
    box_idx = np.random.randint(0, 3)
    white, black = boxes[box_idx]

    # Витягування кулі
    is_white = np.random.random() < white / (white + black)
    results.append(is_white)

p_white = sum(results) / n_trials
print(f"Ймовірність витягнути білу кулю: {p_white:.6f} ≈ 3/5 = {3/5:.6f}")
```

## Формула Байєса

Формула Байєса (або теорема Байєса) дозволяє "перевернути" умовну ймовірність, тобто знайти P(B|A), знаючи P(A|B).

$$P(B|A) = \frac{P(A|B) \cdot P(B)}{P(A)}$$

Часто формулу Байєса використовують у поєднанні з формулою повної ймовірності:

$$P(B_i|A) = \frac{P(A|B_i) \cdot P(B_i)}{\sum_{j=1}^{n} P(A|B_j) \cdot P(B_j)}$$

### Приклад застосування формули Байєса

Використаємо попередній приклад з трьома ящиками, але тепер припустимо, що ми витягнули білу кулю і хочемо знайти ймовірність того, що вона була витягнута з ящика 2.

Нехай:

-   A: витягнута куля біла
-   B₂: вибрано ящик 2

Використовуючи формулу Байєса:
$$P(B_2|A) = \frac{P(A|B_2) \cdot P(B_2)}{P(A)} = \frac{P(A|B_2) \cdot P(B_2)}{P(A|B_1) \cdot P(B_1) + P(A|B_2) \cdot P(B_2) + P(A|B_3) \cdot P(B_3)} = \frac{\frac{4}{5} \cdot \frac{1}{3}}{\frac{2}{5} \cdot \frac{1}{3} + \frac{4}{5} \cdot \frac{1}{3} + \frac{3}{5} \cdot \frac{1}{3}} = \frac{\frac{4}{15}}{\frac{9}{15}} = \frac{4}{9} \approx 0.444$$

```python
# Перевірка за допомогою симуляції
box_choices = []
for _ in range(n_trials):
    # Вибір ящика
    box_idx = np.random.randint(0, 3)
    white, black = boxes[box_idx]

    # Витягування кулі
    is_white = np.random.random() < white / (white + black)

    # Зберігаємо ящик, якщо витягнута біла куля
    if is_white:
        box_choices.append(box_idx)

# Ймовірність, що біла куля з ящика 2 (індекс 1)
p_box2_given_white = box_choices.count(1) / len(box_choices)
print(f"Ймовірність, що біла куля з ящика 2: {p_box2_given_white:.6f} ≈ 4/9 = {4/9:.6f}")
```

### Байєсівська статистика та машинне навчання

Формула Байєса є основою байєсівської статистики та багатьох методів машинного навчання, таких як наївний байєсівський класифікатор.

У контексті машинного навчання формула Байєса записується як:

$$P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}$$

де:

-   C — клас (гіпотеза)
-   X — спостереження (дані)
-   P(C) — апріорна ймовірність класу
-   P(X|C) — правдоподібність даних за умови класу
-   P(C|X) — апостеріорна ймовірність класу після спостереження даних
-   P(X) — загальна ймовірність спостереження (нормалізуючий коефіцієнт)

## Незалежні події

Дві події A і B називаються незалежними, якщо настання однієї з них не впливає на ймовірність настання іншої. Математично це визначається як:

$$P(A|B) = P(A) \text{ або } P(B|A) = P(B)$$

Еквівалентно, події A і B незалежні, якщо:

$$P(A \cap B) = P(A) \cdot P(B)$$

### Приклад незалежних подій

Розглянемо експеримент з двох підкидань монети. Нехай:

-   A: перше підкидання — орел
-   B: друге підкидання — орел

Оскільки результати підкидань не впливають один на одного:

-   P(A) = P(B) = 1/2
-   P(A|B) = P(A) = 1/2
-   P(B|A) = P(B) = 1/2
-   P(A ∩ B) = P(A) · P(B) = 1/2 · 1/2 = 1/4

```python
# Перевірка за допомогою симуляції
np.random.seed(42)
n_trials = 100000

# Симуляція двох підкидань монети
results = []
for _ in range(n_trials):
    first_toss = np.random.choice(['орел', 'решка'])
    second_toss = np.random.choice(['орел', 'решка'])
    results.append((first_toss, second_toss))

# Обчислення ймовірностей
p_first_heads = sum(first == 'орел' for first, _ in results) / n_trials
p_second_heads = sum(second == 'орел' for _, second in results) / n_trials
p_both_heads = sum(first == 'орел' and second == 'орел' for first, second in results) / n_trials

print(f"P(перший орел) = {p_first_heads:.6f}")
print(f"P(другий орел) = {p_second_heads:.6f}")
print(f"P(обидва орли) = {p_both_heads:.6f}")
print(f"P(перший орел) · P(другий орел) = {p_first_heads * p_second_heads:.6f}")
```

### Умовна незалежність

Події A і B називаються умовно незалежними відносно події C, якщо:

$$P(A \cap B|C) = P(A|C) \cdot P(B|C)$$

Умовна незалежність не означає безумовну незалежність і навпаки.

### Незалежні випробування

Послідовність випробувань називається незалежною, якщо результат кожного випробування не залежить від результатів попередніх. Це є основою для багатьох статистичних моделей, таких як схема Бернуллі.

## Випадкові величини

Випадкова величина — це функція, яка кожному елементарному результату випадкового експерименту ставить у відповідність певне числове значення.

### Типи випадкових величин

1. **Дискретна випадкова величина** — може приймати лише скінченну або зліченну кількість значень.
   Приклади: кількість успіхів у серії випробувань Бернуллі, кількість клієнтів у черзі.

2. **Неперервна випадкова величина** — може приймати будь-яке значення з деякого інтервалу.
   Приклади: зріст людини, час очікування, температура.

### Функція розподілу ймовірностей

Функція розподілу ймовірностей (або кумулятивна функція розподілу) F(x) випадкової величини X визначається як:

$$F(x) = P(X \leq x)$$

Вона описує ймовірність того, що випадкова величина прийме значення, не більше за x.

Властивості функції розподілу:

1. 0 ≤ F(x) ≤ 1 для всіх x
2. F(x) — неспадна функція: якщо x₁ < x₂, то F(x₁) ≤ F(x₂)
3. lim(x→-∞) F(x) = 0 і lim(x→∞) F(x) = 1

### Розподіл ймовірностей дискретної випадкової величини

Для дискретної випадкової величини розподіл задається функцією маси ймовірності (probability mass function, PMF):

$$p(x) = P(X = x)$$

Властивості PMF:

1. p(x) ≥ 0 для всіх x
2. Сума p(x) за всіма можливими значеннями x дорівнює 1: ∑ₓ p(x) = 1

### Розподіл ймовірностей неперервної випадкової величини

Для неперервної випадкової величини розподіл задається функцією щільності ймовірності (probability density function, PDF):

$$f(x) = \frac{dF(x)}{dx}$$

Властивості PDF:

1. f(x) ≥ 0 для всіх x
2. Інтеграл f(x) по всій області визначення дорівнює 1: ∫\_{-∞}^{∞} f(x) dx = 1
3. Ймовірність того, що X потрапить в інтервал [a, b]: P(a ≤ X ≤ b) = ∫\_{a}^{b} f(x) dx

### Спільний розподіл випадкових величин

Для декількох випадкових величин можна визначити спільний розподіл, який описує їх одночасну поведінку.

Для двох дискретних випадкових величин X і Y спільний розподіл задається функцією:

$$p(x, y) = P(X = x, Y = y)$$

Для двох неперервних випадкових величин X і Y спільний розподіл задається функцією щільності:

$$f(x, y) = \frac{\partial^2 F(x, y)}{\partial x \partial y}$$

де F(x, y) = P(X ≤ x, Y ≤ y) — спільна функція розподілу.

## Математичне сподівання

Математичне сподівання (або очікуване значення) випадкової величини — це середньозважене значення всіх можливих значень випадкової величини, де вагами є ймовірності цих значень.

### Математичне сподівання дискретної випадкової величини

Для дискретної випадкової величини X з функцією маси ймовірності p(x) математичне сподівання визначається як:

$$E[X] = \sum_{x} x \cdot p(x)$$

### Математичне сподівання неперервної випадкової величини

Для неперервної випадкової величини X з функцією щільності ймовірності f(x) математичне сподівання визначається як:

$$E[X] = \int_{-\infty}^{\infty} x \cdot f(x) dx$$

### Властивості математичного сподівання

1. Лінійність: E[aX + bY] = aE[X] + bE[Y], де a і b — константи
2. Якщо X і Y незалежні: E[XY] = E[X] · E[Y]
3. Для будь-якої функції g(X): E[g(X)] = ∑ₓ g(x) · p(x) (для дискретного випадку) або E[g(X)] = ∫\_{-∞}^{∞} g(x) · f(x) dx (для неперервного випадку)

### Приклад обчислення математичного сподівання

Розглянемо дискретну випадкову величину X — кількість очок при киданні справедливого грального кубика:

-   X може приймати значення 1, 2, 3, 4, 5, 6
-   Кожне значення має ймовірність 1/6

Математичне сподівання:
$$E[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} = \frac{21}{6} = 3.5$$

```python
# Перевірка за допомогою симуляції
np.random.seed(42)
n_trials = 100000

# Кидання кубика
dice_rolls = np.random.randint(1, 7, n_trials)
mean_value = np.mean(dice_rolls)

print(f"Очікуване значення при киданні кубика: {mean_value:.6f} ≈ 3.5")
```

## Дисперсія та стандартне відхилення

Дисперсія випадкової величини — це міра розсіювання значень випадкової величини навколо її математичного сподівання.

### Дисперсія

Дисперсія дискретної випадкової величини X з математичним сподіванням μ = E[X] визначається як:

$$Var(X) = E[(X - \mu)^2] = \sum_{x} (x - \mu)^2 \cdot p(x)$$

Альтернативна формула, яка часто використовується для обчислень:

$$Var(X) = E[X^2] - (E[X])^2$$

Для неперервної випадкової величини:

$$Var(X) = \int_{-\infty}^{\infty} (x - \mu)^2 \cdot f(x) dx = E[X^2] - (E[X])^2$$

### Стандартне відхилення

Стандартне відхилення — це квадратний корінь з дисперсії:

$$\sigma_X = \sqrt{Var(X)}$$

Стандартне відхилення має ту ж розмірність, що й сама випадкова величина, що робить його більш інтуїтивно зрозумілим для інтерпретації.

### Властивості дисперсії

1. Var(X) ≥ 0 для будь-якої випадкової величини X
2. Var(a + bX) = b² · Var(X), де a і b — константи
3. Якщо X і Y незалежні: Var(X + Y) = Var(X) + Var(Y)

### Приклад обчислення дисперсії та стандартного відхилення

Для випадкової величини X — кількості очок при киданні кубика:

-   E[X] = 3.5 (як ми обчислили раніше)
-   E[X²] = 1² · (1/6) + 2² · (1/6) + 3² · (1/6) + 4² · (1/6) + 5² · (1/6) + 6² · (1/6) = (1 + 4 + 9 + 16 + 25 + 36) / 6 = 91 / 6 ≈ 15.167

Дисперсія:
$$Var(X) = E[X^2] - (E[X])^2 = 15.167 - 3.5^2 = 15.167 - 12.25 = 2.917$$

Стандартне відхилення:
$$\sigma_X = \sqrt{Var(X)} = \sqrt{2.917} \approx 1.708$$

```python
# Перевірка за допомогою симуляції
var_value = np.var(dice_rolls)
std_value = np.std(dice_rolls)

print(f"Дисперсія при киданні кубика: {var_value:.6f} ≈ 2.917")
print(f"Стандартне відхилення при киданні кубика: {std_value:.6f} ≈ 1.708")
```

## Функції розподілу ймовірностей

Розподіл ймовірностей описує, як розподілені значення випадкової величини. Він може бути заданий різними способами, залежно від типу випадкової величини.

### Функції розподілу дискретних випадкових величин

1. **Функція маси ймовірності (PMF)**:
   $$p(x) = P(X = x)$$

2. **Кумулятивна функція розподілу (CDF)**:
   $$F(x) = P(X \leq x) = \sum_{t \leq x} p(t)$$

### Функції розподілу неперервних випадкових величин

1. **Функція щільності ймовірності (PDF)**:
   $$f(x) = \frac{dF(x)}{dx}$$

2. **Кумулятивна функція розподілу (CDF)**:
   $$F(x) = P(X \leq x) = \int_{-\infty}^{x} f(t) dt$$

### Важливі розподіли ймовірностей

#### Дискретні розподіли:

1. **Розподіл Бернуллі** — описує експеримент з двома можливими результатами: "успіх" з ймовірністю p і "невдача" з ймовірністю 1-p.

    - PMF: P(X = 1) = p, P(X = 0) = 1 - p
    - E[X] = p
    - Var(X) = p(1 - p)

2. **Біноміальний розподіл** — описує кількість успіхів у n незалежних випробуваннях Бернуллі з ймовірністю успіху p.

    - PMF: P(X = k) = C(n,k) · p^k · (1-p)^(n-k)
    - E[X] = np
    - Var(X) = np(1 - p)

3. **Геометричний розподіл** — описує кількість випробувань Бернуллі до першого успіху.

    - PMF: P(X = k) = (1-p)^(k-1) · p
    - E[X] = 1/p
    - Var(X) = (1-p)/p²

4. **Розподіл Пуассона** — описує кількість подій, що відбуваються за фіксований інтервал часу або в фіксованому просторі, якщо події відбуваються з відомою середньою швидкістю і незалежно одна від одної.
    - PMF: P(X = k) = (λ^k · e^(-λ)) / k!
    - E[X] = λ
    - Var(X) = λ

#### Неперервні розподіли:

1. **Рівномірний розподіл** — описує випадкову величину, яка з однаковою ймовірністю приймає будь-яке значення з деякого інтервалу [a, b].

    - PDF: f(x) = 1/(b-a), якщо a ≤ x ≤ b; 0 в іншому випадку
    - E[X] = (a + b)/2
    - Var(X) = (b - a)²/12

2. **Нормальний (гауссівський) розподіл** — один з найважливіших розподілів у статистиці, характеризується параметрами μ (середнє) і σ² (дисперсія).

    - PDF: f(x) = (1/(σ√(2π))) · e^(-(x-μ)²/(2σ²))
    - E[X] = μ
    - Var(X) = σ²

3. **Експоненціальний розподіл** — описує час між подіями в процесі Пуассона.
    - PDF: f(x) = λe^(-λx), якщо x ≥ 0; 0 в іншому випадку
    - E[X] = 1/λ
    - Var(X) = 1/λ²

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Генерація даних для різних розподілів
x = np.linspace(-5, 15, 1000)

# Біноміальний розподіл
n, p = 10, 0.5
binomial_pmf = stats.binom.pmf(np.arange(n+1), n, p)

# Розподіл Пуассона
lam = 3
poisson_pmf = stats.poisson.pmf(np.arange(15), lam)

# Нормальний розподіл
mu, sigma = 5, 2
normal_pdf = stats.norm.pdf(x, mu, sigma)

# Експоненціальний розподіл
scale = 2  # scale = 1/λ
exponential_pdf = stats.expon.pdf(x, scale=scale)

# Візуалізація
plt.figure(figsize=(15, 10))

# Біноміальний розподіл
plt.subplot(2, 2, 1)
plt.stem(np.arange(n+1), binomial_pmf, basefmt=' ')
plt.title(f'Біноміальний розподіл (n={n}, p={p})')
plt.xlabel('k')
plt.ylabel('P(X = k)')

# Розподіл Пуассона
plt.subplot(2, 2, 2)
plt.stem(np.arange(15), poisson_pmf, basefmt=' ')
plt.title(f'Розподіл Пуассона (λ={lam})')
plt.xlabel('k')
plt.ylabel('P(X = k)')

# Нормальний розподіл
plt.subplot(2, 2, 3)
plt.plot(x, normal_pdf)
plt.title(f'Нормальний розподіл (μ={mu}, σ={sigma})')
plt.xlabel('x')
plt.ylabel('f(x)')

# Експоненціальний розподіл
plt.subplot(2, 2, 4)
plt.plot(x, exponential_pdf)
plt.title(f'Експоненціальний розподіл (λ={1/scale})')
plt.xlabel('x')
plt.ylabel('f(x)')

plt.tight_layout()
plt.show()
```

## Практичне застосування в аналізі даних

Теорія ймовірності є фундаментом для багатьох методів аналізу даних та машинного навчання.

### Оцінка ризиків та прогнозування

Теорія ймовірності дозволяє оцінювати ризики та робити прогнози в умовах невизначеності. Наприклад:

-   **Фінанси**: оцінка ризику інвестицій, аналіз страхових випадків
-   **Медицина**: оцінка ризику захворювань, ефективності лікування
-   **Маркетинг**: прогнозування поведінки споживачів, ефективності рекламних кампаній

### Байєсівський аналіз

Байєсівський підхід дозволяє оновлювати наші знання про ймовірності, базуючись на нових даних:

```python
# Приклад: Байєсівське оновлення ймовірності
# Припустимо, ми оцінюємо ймовірність того, що монета нечесна (ймовірність випадання орла p ≠ 0.5)

# Початкова (апріорна) ймовірність того, що монета нечесна
p_unfair_prior = 0.1

# Правдоподібність: Ймовірність отримати 8 орлів з 10 підкидань, якщо монета...
# ... чесна (p = 0.5)
p_data_given_fair = stats.binom.pmf(8, 10, 0.5)
# ... нечесна (припустимо, p = 0.8)
p_data_given_unfair = stats.binom.pmf(8, 10, 0.8)

# Загальна ймовірність отримання таких даних
p_data = p_data_given_fair * (1 - p_unfair_prior) + p_data_given_unfair * p_unfair_prior

# Апостеріорна ймовірність того, що монета нечесна
p_unfair_posterior = (p_data_given_unfair * p_unfair_prior) / p_data

print(f"Апріорна ймовірність нечесної монети: {p_unfair_prior:.4f}")
print(f"Апостеріорна ймовірність нечесної монети: {p_unfair_posterior:.4f}")
```

### Статистичні тести

Теорія ймовірності є основою для статистичних тестів, які використовуються для перевірки гіпотез про дані:

```python
# Приклад: t-тест для порівняння середніх двох вибірок
from scipy import stats

# Генерація даних для двох груп
np.random.seed(42)
group1 = np.random.normal(loc=10, scale=2, size=100)
group2 = np.random.normal(loc=11, scale=2, size=100)

# Проведення t-тесту
t_stat, p_value = stats.ttest_ind(group1, group2)

print(f"t-статистика: {t_stat:.4f}")
print(f"p-значення: {p_value:.4f}")
print(f"Висновок: {'Відхиляємо' if p_value < 0.05 else 'Не відхиляємо'} нульову гіпотезу про рівність середніх")
```

### Моделювання та симуляції

Теорія ймовірності дозволяє створювати математичні моделі та проводити симуляції для розуміння складних систем:

```python
# Приклад: Симуляція Монте-Карло для оцінки π
np.random.seed(42)
n_points = 100000

# Генерація випадкових точок в квадраті [0, 1] x [0, 1]
x = np.random.random(n_points)
y = np.random.random(n_points)

# Перевірка, чи точка потрапляє в чверть кола
inside_circle = (x**2 + y**2) <= 1

# Оцінка π = 4 * (кількість точок всередині чверті кола / загальна кількість точок)
pi_estimate = 4 * np.mean(inside_circle)

print(f"Оцінка π методом Монте-Карло: {pi_estimate:.6f}")
print(f"Точне значення π: {np.pi:.6f}")
print(f"Помилка: {abs(pi_estimate - np.pi):.6f}")
```

### Машинне навчання

Багато алгоритмів машинного навчання базуються на принципах теорії ймовірності:

-   **Наївний Байєсівський класифікатор**: використовує формулу Байєса для класифікації
-   **Логістична регресія**: моделює ймовірність належності до класу
-   **Прихована Марківська модель**: моделює послідовності з прихованими станами
-   **Гауссівські процеси**: використовуються для регресії та класифікації

```python
# Приклад: Наївний Байєсівський класифікатор
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Генерація синтетичних даних
np.random.seed(42)
n_samples = 1000
X = np.random.randn(n_samples, 2)  # Два ознаки
y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Клас залежить від суми ознак

# Додаємо шум
y = np.logical_xor(y, np.random.random(n_samples) < 0.1).astype(int)

# Розділення на навчальну та тестову вибірки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Навчання наївного байєсівського класифікатора
model = GaussianNB()
model.fit(X_train, y_train)

# Прогнозування
y_pred = model.predict(X_test)

# Оцінка точності
accuracy = accuracy_score(y_test, y_pred)
print(f"Точність наївного байєсівського класифікатора: {accuracy:.4f}")
```

## Висновки

Теорія ймовірності є фундаментальною дисципліною, яка дозволяє кількісно описати та аналізувати випадкові події та процеси. Основні концепції теорії ймовірності, такі як умовна ймовірність, незалежність подій, випадкові величини та їх розподіли, мають широке застосування в аналізі даних, статистиці та машинному навчанні.

Розуміння теорії ймовірності дозволяє:

1. **Моделювати невизначеність** — описувати та кількісно оцінювати випадкові процеси
2. **Робити обґрунтовані висновки** — використовувати дані для перевірки гіпотез та оновлення наших знань
3. **Приймати оптимальні рішення** — враховувати ризики та невизначеність при прийнятті рішень
4. **Розуміти складні системи** — моделювати взаємодію багатьох випадкових факторів

У наступному розділі ми розглянемо розподіли ймовірностей більш детально, зосереджуючись на їх властивостях, застосуваннях та зв'язках з реальними даними.

---

**Попередня тема:** [Описова статистика](./04_описова_статистика.md)

**Наступна тема:** [Розподіли ймовірностей](./06_розподіли_ймовірностей.md)
