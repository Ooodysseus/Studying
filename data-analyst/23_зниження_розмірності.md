# Зниження розмірності даних

[⬅️ Попередня тема](./22_кореляційний_аналіз.md) | [⬆️ Зміст](./інструкції.md) | [➡️ Наступна тема](./24_вступ_до_ml.md)

## Зміст розділу

-   [Вступ до зниження розмірності](#вступ-до-зниження-розмірності)
-   [Проблема "прокляття розмірності"](#проблема-прокляття-розмірності)
-   [Підходи до зниження розмірності](#підходи-до-зниження-розмірності)
-   [Метод головних компонент (PCA)](#метод-головних-компонент-pca)
-   [Факторний аналіз](#факторний-аналіз)
-   [Багатовимірне шкалювання (MDS)](#багатовимірне-шкалювання-mds)
-   [t-SNE (t-distributed Stochastic Neighbor Embedding)](#t-sne-t-distributed-stochastic-neighbor-embedding)
-   [UMAP (Uniform Manifold Approximation and Projection)](#umap-uniform-manifold-approximation-and-projection)
-   [Автокодувальники (Autoencoders)](#автокодувальники-autoencoders)
-   [Відбір ознак (Feature Selection)](#відбір-ознак-feature-selection)
-   [Порівняння методів зниження розмірності](#порівняння-методів-зниження-розмірності)
-   [Практичні приклади застосування](#практичні-приклади-застосування)
-   [Оцінка якості зниження розмірності](#оцінка-якості-зниження-розмірності)

## Вступ до зниження розмірності

[Буде наповнено: Визначення поняття "зниження розмірності". Основні причини для застосування цих методів: візуалізація, зниження складності моделей, видалення шуму в даних. Короткий історичний огляд розвитку методів.]

### Важливість зниження розмірності в аналізі даних

[Буде наповнено: Роль методів зниження розмірності в сучасному аналізі даних. Причини потреби в цих методах при роботі з великими наборами даних.]

### Теоретичні основи

[Буде наповнено: Загальні математичні принципи, на яких базуються методи зниження розмірності. Концепція проекцій у багатовимірному просторі.]

## Проблема "прокляття розмірності"

[Буде наповнено: Детальний опис проблеми "прокляття розмірності" (curse of dimensionality). Як зростання кількості вимірів впливає на аналіз даних та машинне навчання.]

### Геометричні особливості багатовимірних просторів

[Буде наповнено: Парадокси багатовимірних просторів. Зміна поведінки відстаней у багатовимірних просторах. Розрідженість даних.]

### Наслідки для аналізу та моделювання

[Буде наповнено: Як висока розмірність погіршує продуктивність алгоритмів машинного навчання. Проблеми з вимірюванням схожості. Збільшення обчислювальної складності.]

## Підходи до зниження розмірності

[Буде наповнено: Огляд двох основних категорій методів: методи вибору ознак (feature selection) та методи вилучення ознак (feature extraction). Їх порівняння та контрасти.]

### Лінійні vs нелінійні методи

[Буде наповнено: Порівняння лінійних (PCA, LDA) та нелінійних (t-SNE, UMAP) методів зниження розмірності. Коли застосовувати кожен з них.]

### Вибір кількості вимірів

[Буде наповнено: Методики визначення оптимальної кількості вимірів для збереження. Компроміс між зниженням розмірності та збереженням інформації.]

## Метод головних компонент (PCA)

[Буде наповнено: Детальний опис методу головних компонент. Математичне обґрунтування. Історія та розвиток методу.]

### Математичне підґрунтя PCA

[Буде наповнено: Математичне формулювання PCA. Власні вектори та власні значення матриці коваріації. Процес максимізації дисперсії.]

```python
# Приклад застосування PCA
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

# Завантаження датасету Iris
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Стандартизація даних
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Застосування PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Відсоток збереженої дисперсії
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

# Візуалізація пояснення дисперсії
plt.figure(figsize=(10, 6))
plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, label='Individual explained variance')
plt.step(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, where='mid', label='Cumulative explained variance')
plt.axhline(y=0.95, linestyle='--', color='r', label='95% explained variance threshold')
plt.xlabel('Головні компоненти')
plt.ylabel('Пояснена дисперсія')
plt.title('Пояснена дисперсія для кожної головної компоненти')
plt.legend()
plt.tight_layout()
plt.show()

# Візуалізація даних у просторі перших двох головних компонент
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], alpha=0.8, label=target_name)
plt.xlabel('Перша головна компонента')
plt.ylabel('Друга головна компонента')
plt.title('PCA проекція датасету Iris')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Інтерпретація головних компонент

[Буде наповнено: Як інтерпретувати головні компоненти. Аналіз навантажень (loadings). Біплот для візуалізації.]

```python
# Приклад візуалізації навантажень головних компонент (біплот)
plt.figure(figsize=(10, 8))
# Проекція даних
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], alpha=0.6, label=target_name)

# Відображення навантажень
for i, (name, coef) in enumerate(zip(feature_names, pca.components_.T)):
    plt.arrow(0, 0, coef[0]*5, coef[1]*5, color='r', alpha=0.7)
    plt.text(coef[0]*5.2, coef[1]*5.2, name, color='g', ha='center', va='center')

plt.xlabel('Перша головна компонента')
plt.ylabel('Друга головна компонента')
plt.title('Біплот: проекція даних та навантаження ознак')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Обмеження PCA

[Буде наповнено: Обмеження методу головних компонент. Випадки, коли PCA не є ефективним методом зниження розмірності.]

## Факторний аналіз

[Буде наповнено: Детальний опис факторного аналізу. Порівняння з PCA. Теоретичні основи та практичне застосування.]

### Модель факторного аналізу

[Буде наповнено: Математична модель факторного аналізу. Спільні фактори та специфічні фактори. Факторні навантаження.]

```python
# Приклад факторного аналізу
from sklearn.decomposition import FactorAnalysis

# Застосування факторного аналізу
n_factors = 2
fa = FactorAnalysis(n_components=n_factors, random_state=42)
X_fa = fa.fit_transform(X_scaled)

# Візуалізація результатів
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_fa[y == i, 0], X_fa[y == i, 1], alpha=0.8, label=target_name)
plt.xlabel('Перший фактор')
plt.ylabel('Другий фактор')
plt.title('Проекція даних у просторі факторів')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Аналіз факторних навантажень
factor_loadings = fa.components_.T
loadings_df = pd.DataFrame(
    factor_loadings,
    columns=[f'Factor {i+1}' for i in range(n_factors)],
    index=feature_names
)
print("Факторні навантаження:")
print(loadings_df)
```

### Методи обертання факторів

[Буде наповнено: Концепція обертання факторів для покращення інтерпретації. Варимакс, промакс та інші методи обертання.]

### Порівняння факторного аналізу з PCA

[Буде наповнено: Ключові відмінності між факторним аналізом та методом головних компонент. Випадки, коли один метод має перевагу над іншим.]

## Багатовимірне шкалювання (MDS)

[Буде наповнено: Опис багатовимірного шкалювання. Принципи збереження відстаней між об'єктами при проекції.]

### Класичне MDS та метричне MDS

[Буде наповнено: Порівняння класичного та метричного MDS. Математичні основи та алгоритми.]

```python
# Приклад застосування MDS
from sklearn.manifold import MDS

# Застосування MDS
mds = MDS(n_components=2, random_state=42)
X_mds = mds.fit_transform(X_scaled)

# Візуалізація результатів
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_mds[y == i, 0], X_mds[y == i, 1], alpha=0.8, label=target_name)
plt.xlabel('Перша координата MDS')
plt.ylabel('Друга координата MDS')
plt.title('Проекція даних за допомогою MDS')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Неметричне MDS

[Буде наповнено: Опис неметричного MDS. Використання для порядкових даних. Алгоритм Крускала.]

### Застосування MDS для візуалізації

[Буде наповнено: Практичні випадки застосування MDS для візуалізації подібності об'єктів. Інтерпретація результатів.]

## t-SNE (t-distributed Stochastic Neighbor Embedding)

[Буде наповнено: Детальний опис t-SNE як потужного методу нелінійного зниження розмірності. Принципи роботи та теоретичні основи.]

### Алгоритм t-SNE

[Буде наповнено: Опис алгоритму t-SNE. Перетворення відстаней у ймовірності. Використання t-розподілу для вирішення проблеми скупченості.]

```python
# Приклад застосування t-SNE
from sklearn.manifold import TSNE

# Застосування t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
X_tsne = tsne.fit_transform(X_scaled)

# Візуалізація результатів
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_tsne[y == i, 0], X_tsne[y == i, 1], alpha=0.8, label=target_name)
plt.xlabel('Перша координата t-SNE')
plt.ylabel('Друга координата t-SNE')
plt.title('Проекція даних за допомогою t-SNE')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Вплив гіперпараметрів t-SNE

[Буде наповнено: Аналіз впливу різних параметрів t-SNE (perplexity, learning rate, number of iterations) на результати проекції.]

```python
# Приклад дослідження впливу параметра perplexity
perplexities = [5, 30, 50, 100]
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.flatten()

for i, perplexity in enumerate(perplexities):
    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, n_iter=1000)
    X_tsne = tsne.fit_transform(X_scaled)

    for j, target_name in enumerate(iris.target_names):
        axes[i].scatter(X_tsne[y == j, 0], X_tsne[y == j, 1], alpha=0.8, label=target_name)

    axes[i].set_title(f'perplexity = {perplexity}')
    axes[i].set_xlabel('Перша координата t-SNE')
    axes[i].set_ylabel('Друга координата t-SNE')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Обмеження t-SNE

[Буде наповнено: Обмеження методу t-SNE. Проблеми з інтерпретацією глобальної структури даних. Обчислювальна складність.]

## UMAP (Uniform Manifold Approximation and Projection)

[Буде наповнено: Опис методу UMAP як сучасної альтернативи t-SNE. Теоретичне підґрунтя, засноване на теорії категорій та топології.]

### Теоретичні основи UMAP

[Буде наповнено: Математичні основи методу UMAP. Побудова нечітких топологічних уявлень. Оптимізація через силові алгоритми.]

```python
# Приклад застосування UMAP
import umap

# Застосування UMAP
reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
X_umap = reducer.fit_transform(X_scaled)

# Візуалізація результатів
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_umap[y == i, 0], X_umap[y == i, 1], alpha=0.8, label=target_name)
plt.xlabel('Перша координата UMAP')
plt.ylabel('Друга координата UMAP')
plt.title('Проекція даних за допомогою UMAP')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Параметри та оптимізація UMAP

[Буде наповнено: Вплив параметрів UMAP (n_neighbors, min_dist, metric) на результати проекції. Рекомендації щодо вибору параметрів.]

### Порівняння UMAP з t-SNE

[Буде наповнено: Детальне порівняння UMAP та t-SNE з точки зору якості проекції, швидкості та масштабованості.]

## Автокодувальники (Autoencoders)

[Буде наповнено: Опис автокодувальників як методу зниження розмірності на основі нейронних мереж. Архітектура та принципи роботи.]

### Архітектура автокодувальника

[Буде наповнено: Детальний опис архітектури автокодувальника. Кодер, декодер, прихований шар. Види функцій активації.]

```python
# Приклад реалізації автокодувальника з використанням TensorFlow/Keras
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Підготовка даних
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
y_train_cat = to_categorical(y_train)
y_test_cat = to_categorical(y_test)

# Параметри автокодувальника
input_dim = X_train.shape[1]
encoding_dim = 2
activation_func = 'relu'
output_activation = 'linear'

# Побудова моделі автокодувальника
input_layer = Input(shape=(input_dim,))
# Кодер
encoded = Dense(encoding_dim, activation=activation_func)(input_layer)
# Декодер
decoded = Dense(input_dim, activation=output_activation)(encoded)

# Створення моделей
autoencoder = Model(inputs=input_layer, outputs=decoded)
encoder = Model(inputs=input_layer, outputs=encoded)

# Компіляція моделі
autoencoder.compile(optimizer='adam', loss='mse')

# Навчання моделі
history = autoencoder.fit(
    X_train, X_train,
    epochs=50,
    batch_size=16,
    shuffle=True,
    validation_data=(X_test, X_test),
    verbose=0
)

# Візуалізація процесу навчання
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Процес навчання автокодувальника')
plt.xlabel('Епохи')
plt.ylabel('Функція втрат (MSE)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Отримання кодованого представлення даних
encoded_data = encoder.predict(X_scaled)

# Візуалізація результатів
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(encoded_data[y == i, 0], encoded_data[y == i, 1], alpha=0.8, label=target_name)
plt.xlabel('Перша координата автокодувальника')
plt.ylabel('Друга координата автокодувальника')
plt.title('Проекція даних за допомогою автокодувальника')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Варіаційні автокодувальники (VAE)

[Буде наповнено: Опис варіаційних автокодувальників. Генеративна складова VAE. Функція втрат із регуляризацією KL-дивергенцією.]

### Застосування для зниження розмірності та генерації даних

[Буде наповнено: Використання автокодувальників для зниження розмірності та генерації нових даних. Переваги перед іншими методами.]

## Відбір ознак (Feature Selection)

[Буде наповнено: Огляд методів відбору ознак як альтернативи зниженню розмірності. Порівняння з методами вилучення ознак.]

### Фільтраційні методи

[Буде наповнено: Опис фільтраційних методів відбору ознак. Використання статистичних метрик (кореляція, взаємна інформація) для ранжування ознак.]

```python
# Приклад застосування фільтраційних методів
from sklearn.feature_selection import SelectKBest, f_classif

# Застосування фільтраційного методу (ANOVA F-value)
selector = SelectKBest(f_classif, k=2)
X_filtered = selector.fit_transform(X, y)
selected_features_idx = selector.get_support(indices=True)
selected_features = [feature_names[i] for i in selected_features_idx]

print(f"Відібрані ознаки: {', '.join(selected_features)}")
print(f"Оцінки важливості: {selector.scores_[selected_features_idx]}")

# Візуалізація результатів
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_filtered[y == i, 0], X_filtered[y == i, 1], alpha=0.8, label=target_name)
plt.xlabel(selected_features[0])
plt.ylabel(selected_features[1])
plt.title('Проекція даних на відібрані ознаки')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Методи обгортки (Wrapper Methods)

[Буде наповнено: Опис методів обгортки для відбору ознак. Рекурсивне вилучення ознак (RFE), послідовний відбір ознак.]

```python
# Приклад рекурсивного вилучення ознак (RFE)
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# Застосування RFE
estimator = RandomForestClassifier(n_estimators=10, random_state=42)
selector = RFE(estimator, n_features_to_select=2, step=1)
selector = selector.fit(X, y)

selected_features_idx = np.where(selector.support_)[0]
selected_features = [feature_names[i] for i in selected_features_idx]
print(f"Відібрані ознаки (RFE): {', '.join(selected_features)}")
print(f"Ранжування ознак: {selector.ranking_}")

# Візуалізація результатів
X_rfe = X[:, selected_features_idx]
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_rfe[y == i, 0], X_rfe[y == i, 1], alpha=0.8, label=target_name)
plt.xlabel(selected_features[0])
plt.ylabel(selected_features[1])
plt.title('Проекція даних на ознаки, відібрані за допомогою RFE')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Вбудовані методи (Embedded Methods)

[Буде наповнено: Опис вбудованих методів відбору ознак. L1-регуляризація, важливість ознак у дерев'яних моделях, LASSO.]

```python
# Приклад використання вбудованих методів (важливість ознак у Random Forest)
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Навчання моделі
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# Отримання важливості ознак
feature_importance = rf.feature_importances_
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importance
}).sort_values(by='Importance', ascending=False)

print("Важливість ознак за Random Forest:")
print(feature_importance_df)

# Візуалізація важливості ознак
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важливість')
plt.ylabel('Ознаки')
plt.title('Важливість ознак за Random Forest')
plt.tight_layout()
plt.show()
```

## Порівняння методів зниження розмірності

[Буде наповнено: Комплексне порівняння різних методів зниження розмірності за різними критеріями: якість збереження структури даних, обчислювальна складність, інтерпретованість.]

### Візуальне порівняння

[Буде наповнено: Візуальне порівняння результатів різних методів зниження розмірності на одному наборі даних.]

```python
# Приклад візуального порівняння методів зниження розмірності
from sklearn.manifold import Isomap

# Застосування різних методів до одного набору даних
methods = {
    'PCA': PCA(n_components=2, random_state=42),
    'MDS': MDS(n_components=2, random_state=42),
    't-SNE': TSNE(n_components=2, perplexity=30, random_state=42),
    'Isomap': Isomap(n_components=2, n_neighbors=10)
}

# Підготовка візуалізації
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.flatten()

# Застосування та візуалізація кожного методу
for i, (name, method) in enumerate(methods.items()):
    X_transformed = method.fit_transform(X_scaled)

    for j, target_name in enumerate(iris.target_names):
        axes[i].scatter(X_transformed[y == j, 0], X_transformed[y == j, 1], alpha=0.8, label=target_name)

    axes[i].set_title(name)
    axes[i].set_xlabel('Перша координата')
    axes[i].set_ylabel('Друга координата')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Кількісне порівняння

[Буде наповнено: Кількісне порівняння методів за метриками збереження структури даних (коефіцієнт силуету, збереження відстаней, тощо).]

## Практичні приклади застосування

[Буде наповнено: Детальні приклади застосування методів зниження розмірності для різних практичних завдань.]

### Обробка зображень

[Буде наповнено: Застосування зниження розмірності для аналізу та класифікації зображень. Особливості роботи з великими наборами зображень.]

### Аналіз текстових даних

[Буде наповнено: Застосування зниження розмірності для аналізу текстових даних. Візуалізація семантичних просторів. Робота з векторними представленнями слів.]

### Аналіз генетичних даних

[Буде наповнено: Застосування зниження розмірності для аналізу високорозмірних генетичних даних. Візуалізація кластерів генів або зразків.]

## Оцінка якості зниження розмірності

[Буде наповнено: Методики оцінки якості результатів зниження розмірності. Критерії для вибору найкращого методу для конкретного завдання.]

### Метрики збереження локальної структури

[Буде наповнено: Метрики для оцінки збереження локальної структури даних при зниженні розмірності (збереження k-найближчих сусідів, локальне збереження відстаней).]

```python
# Приклад оцінки збереження k-найближчих сусідів
from sklearn.neighbors import NearestNeighbors
import numpy as np

def knn_preservation(X_original, X_reduced, k=5):
    """
    Розрахунок метрики збереження k-найближчих сусідів
    """
    # Знаходження k-найближчих сусідів у вихідному просторі
    nn_original = NearestNeighbors(n_neighbors=k+1)  # +1, щоб врахувати сам об'єкт
    nn_original.fit(X_original)
    indices_original = nn_original.kneighbors(X_original, return_distance=False)

    # Знаходження k-найближчих сусідів у зниженому просторі
    nn_reduced = NearestNeighbors(n_neighbors=k+1)  # +1, щоб врахувати сам об'єкт
    nn_reduced.fit(X_reduced)
    indices_reduced = nn_reduced.kneighbors(X_reduced, return_distance=False)

    # Розрахунок метрики збереження
    preservation = []
    for i in range(len(X_original)):
        # Множини сусідів, виключаючи сам об'єкт
        neighbors_original = set(indices_original[i][1:])
        neighbors_reduced = set(indices_reduced[i][1:])
        # Розрахунок міри схожості множин (Жаккара)
        intersection = len(neighbors_original.intersection(neighbors_reduced))
        union = len(neighbors_original.union(neighbors_reduced))
        preservation.append(intersection / union)

    return np.mean(preservation)

# Порівняння методів за збереженням k-найближчих сусідів
methods_reduced = {
    'PCA': pca.fit_transform(X_scaled),
    'MDS': mds.fit_transform(X_scaled),
    't-SNE': tsne.fit_transform(X_scaled)
}

print("Збереження 5-найближчих сусідів:")
for name, X_reduced in methods_reduced.items():
    preservation = knn_preservation(X_scaled, X_reduced, k=5)
    print(f"{name}: {preservation:.4f}")
```

### Метрики збереження глобальної структури

[Буде наповнено: Метрики для оцінки збереження глобальної структури даних (кореляція відстаней, стрес-функція в MDS).]

### Вибір оптимального методу для конкретного завдання

[Буде наповнено: Рекомендації щодо вибору методу зниження розмірності в залежності від типу даних та цілей аналізу.]

---

## Висновки

[Буде наповнено: Узагальнення ключових концепцій розділу. Рекомендації щодо ефективного застосування методів зниження розмірності в різних сферах аналізу даних.]

## Додаткові ресурси

[Буде наповнено: Список рекомендованої літератури та онлайн-ресурсів для поглибленого вивчення методів зниження розмірності.]

---

[⬅️ Попередня тема](./22_кореляційний_аналіз.md) | [⬆️ Зміст](./інструкції.md) | [➡️ Наступна тема](./24_вступ_до_ml.md)
