# Життєвий цикл аналізу даних

## Зміст розділу

-   [Огляд життєвого циклу](#огляд-життєвого-циклу)
-   [Етап 1: Визначення проблеми](#етап-1-визначення-проблеми)
-   [Етап 2: Збір та підготовка даних](#етап-2-збір-та-підготовка-даних)
-   [Етап 3: Дослідження даних](#етап-3-дослідження-даних)
-   [Етап 4: Моделювання та аналіз](#етап-4-моделювання-та-аналіз)
-   [Етап 5: Інтерпретація результатів](#етап-5-інтерпретація-результатів)
-   [Етап 6: Комунікація та впровадження](#етап-6-комунікація-та-впровадження)
-   [Ітеративність процесу](#ітеративність-процесу)

---

## Огляд життєвого циклу

**Життєвий цикл аналізу даних (Data Analysis Lifecycle)** — це структурований підхід до виконання аналітичних проектів, який забезпечує систематичність, якість та відтворюваність результатів.

### Важливість структурованого підходу

**Переваги системного підходу:**

-   **Зменшення ризиків** помилок та упущень
-   **Підвищення якості** аналітичних результатів
-   **Прискорення** виконання проектів за рахунок стандартизації
-   **Покращення комунікації** між учасниками команди
-   **Забезпечення відтворюваності** результатів

**Проблеми ad-hoc підходу:**

-   Хаотичність процесу
-   Дублювання зусиль
-   Пропуск важливих етапів
-   Складність валідації результатів
-   Низька якість документації

### Зв'язок з бізнес-цілями

Кожен етап циклу повинен бути орієнтований на досягнення конкретних бізнес-цілей:

**Стратегічний рівень:**

-   Підтримка стратегічних ініціатив
-   Вимірювання прогресу KPI
-   Ідентифікація нових можливостей

**Тактичний рівень:**

-   Оптимізація поточних процесів
-   Вирішення оперативних проблем
-   Підтримка щоденних рішень

```mermaid
graph TD
    A[Визначення проблеми] --> B[Збір даних]
    B --> C[Підготовка даних]
    C --> D[Дослідження EDA]
    D --> E[Моделювання]
    E --> F[Валідація]
    F --> G[Інтерпретація]
    G --> H[Комунікація]
    H --> I[Впровадження]
    I --> J[Моніторинг]
    J --> A

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#e8f5e8
    style H fill:#f3e5f5
    style I fill:#ffebee
```

### Основні принципи

**1. Ітеративність**

-   Цикл не є лінійним процесом
-   Постійне повернення до попередніх етапів
-   Удосконалення на основі нових insights

**2. Документованість**

-   Фіксація всіх рішень та припущень
-   Збереження коду та методології
-   Можливість відтворення результатів

**3. Якість над швидкістю**

-   Ретельна перевірка даних
-   Валідація результатів
-   Тестування гіпотез

**4. Бізнес-орієнтованість**

-   Фокус на actionable insights
-   Розуміння бізнес-контексту
-   Вимірювання впливу рішень

## Етап 1: Визначення проблеми

**Найважливіший етап** — правильне формулювання проблеми визначає успіх всього проекту. За статистикою, 70% невдалих аналітичних проектів провалюються саме через нечітко сформульовану задачу.

### Важливість правильного формулювання

**Типові помилки:**

-   Занадто загальне формулювання ("покращити продажі")
-   Відсутність конкретних метрик успіху
-   Неврахування обмежень та ресурсів
-   Змішування симптомів з причинами

**Наслідки поганого формулювання:**

-   Витрачання ресурсів на неправильний аналіз
-   Отримання нерелевантних insights
-   Неможливість виміряти успіх проекту
-   Розчарування стейкхолдерів

### Методи збору вимог

**1. Інтерв'ю зі стейкхолдерами**

```
Ключові питання:
- Яка конкретна бізнес-проблема потребує вирішення?
- Які рішення будуть прийняті на основі аналізу?
- Хто є цільовою аудиторією результатів?
- Які ресурси та дані доступні?
- Які обмеження існують (час, бюджет, технічні)?
```

**2. Воркшопи та мозкові штурми**

-   Залучення різних департаментів
-   Вирівнювання очікувань
-   Ідентифікація всіх аспектів проблеми

**3. Аналіз існуючих звітів**

-   Огляд поточної аналітики
-   Виявлення прогалин в знаннях
-   Розуміння контексту

### SMART-критерії для аналітичних завдань

**Specific (Конкретно):**
❌ "Проаналізувати клієнтів"
✅ "Визначити характеристики клієнтів, які мають найвищий lifetime value"

**Measurable (Вимірювано):**
❌ "Покращити конверсію"
✅ "Збільшити конверсію з відвідування на покупку на 2 п.п."

**Achievable (Досяжно):**

-   Чи доступні необхідні дані?
-   Чи вистачає часу та ресурсів?
-   Чи реалістичні очікування?

**Relevant (Релевантно):**

-   Як результати вплинуть на бізнес?
-   Чи відповідає задача стратегічним цілям?

**Time-bound (Обмежено в часі):**

-   Коли потрібні результати?
-   Які проміжні дедлайни?

### Ключові питання для початку проекту

**Бізнес-контекст:**

1. Яка бізнес-проблема або можливість?
2. Як вимірюється успіх?
3. Хто буде використовувати результати?
4. Які альтернативні рішення розглядалися?

**Дані та ресурси:** 5. Які дані доступні та де вони знаходяться? 6. Яка якість існуючих даних? 7. Чи потрібно збирати додаткові дані? 8. Які технічні обмеження існують?

**Результати та дії:** 9. У якому форматі потрібні результати? 10. Які конкретні дії будуть вжиті? 11. Як буде впроваджуватися рішення? 12. Як будемо вимірювати ефект?

### Приклади правильного формулювання

**Поганий приклад:**
"Потрібно проаналізувати продажі"

**Хороший приклад:**
"Ідентифікувати топ-5 факторів, які впливають на місячний обсяг продажів категорії електроніки, щоб збільшити виручку на 15% протягом наступного кварталу через оптимізацію маркетингових бюджетів"

**Деталізація:**

-   **Мета:** Збільшення виручки на 15%
-   **Часовий горизонт:** Наступний квартал
-   **Скоуп:** Категорія електроніки
-   **Метод:** Аналіз факторів впливу
-   **Дія:** Оптимізація маркетингових бюджетів
-   **Метрика успіху:** Зростання місячного обсягу продажів

## Етап 2: Збір та підготовка даних

**Найтрудомісткий етап** — за оцінками експертів, 80% часу аналітика витрачається саме на підготовку даних. Якість вхідних даних напряму впливає на якість аналітичних висновків.

### Джерела даних

**Внутрішні джерела:**

-   **Операційні БД** (CRM, ERP, e-commerce платформи)
-   **Аналітичні сховища** (Data Warehouse, Data Lake)
-   **Веб-аналітика** (Google Analytics, Adobe Analytics)
-   **Файли** (Excel, CSV, JSON, XML)
-   **API** внутрішніх систем

**Зовнішні джерела:**

-   **Публічні дані** (держстатистика, відкриті дані)
-   **Комерційні дані** (Nielsen, Kantar, соціодемографія)
-   **Соціальні мережі** (Twitter API, Facebook Insights)
-   **Погодні дані**, **економічні індикатори**
-   **Партнерські дані** (дані від бізнес-партнерів)

### Методи збору даних

**1. Автоматизоване збирання**

```python
# Приклад підключення до БД
import pandas as pd
import sqlalchemy

# Створення підключення
engine = sqlalchemy.create_engine('postgresql://user:password@host:port/database')

# Витягування даних
query = """
SELECT customer_id, purchase_date, amount, product_category
FROM sales
WHERE purchase_date >= '2024-01-01'
"""
df = pd.read_sql(query, engine)
```

**2. API інтеграція**

```python
# Приклад роботи з API
import requests
import json

# Google Analytics API
def get_ga_data(view_id, start_date, end_date):
    # Аутентифікація та запит до API
    response = requests.get(f"https://analyticsreporting.googleapis.com/v4/reports:batchGet",
                          headers=headers, json=request_body)
    return response.json()
```

**3. Веб-скрапінг**

```python
# Приклад парсингу веб-сторінок
import requests
from bs4 import BeautifulSoup

url = "https://example.com/data"
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')
# Витягування потрібних даних
```

### Data Quality Assessment

**Димензії якості даних:**

**1. Accuracy (Точність)**

-   Відповідність даних реальності
-   Перевірка через зовнішні джерела
-   Валідація бізнес-правил

**2. Completeness (Повнота)**

-   Відсутність пропущених значень
-   Покриття всіх необхідних періодів
-   Наявність усіх обов'язкових полів

**3. Consistency (Консистентність)**

-   Узгодженість між різними джерелами
-   Дотримання форматів та стандартів
-   Відсутність суперечностей

**4. Timeliness (Актуальність)**

-   Свіжість даних
-   Регулярність оновлень
-   Відповідність потребам аналізу

**5. Validity (Валідність)**

-   Відповідність типам даних
-   Дотримання обмежень (діапазони, формати)
-   Логічна коректність значень

### Очищення та трансформація

```python
import pandas as pd
import numpy as np
from datetime import datetime

# Завантаження даних
df = pd.read_csv('sales_data.csv')

# 1. Обробка пропущених значень
# Видалення рядків з критичними пропусками
df = df.dropna(subset=['customer_id', 'purchase_date'])

# Заповнення пропусків медіаною для числових колонок
df['amount'].fillna(df['amount'].median(), inplace=True)

# Заповнення пропусків модою для категоріальних
df['category'].fillna(df['category'].mode()[0], inplace=True)

# 2. Виправлення типів даних
df['purchase_date'] = pd.to_datetime(df['purchase_date'])
df['amount'] = pd.to_numeric(df['amount'], errors='coerce')

# 3. Стандартизація форматів
df['email'] = df['email'].str.lower().str.strip()
df['phone'] = df['phone'].str.replace(r'[^\d]', '', regex=True)

# 4. Виявлення викидів
Q1 = df['amount'].quantile(0.25)
Q3 = df['amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Позначення викидів
df['is_outlier'] = (df['amount'] < lower_bound) | (df['amount'] > upper_bound)

# 5. Створення нових змінних
df['year'] = df['purchase_date'].dt.year
df['month'] = df['purchase_date'].dt.month
df['day_of_week'] = df['purchase_date'].dt.day_name()

# 6. Агрегація даних
monthly_sales = df.groupby(['year', 'month']).agg({
    'amount': ['sum', 'mean', 'count'],
    'customer_id': 'nunique'
}).reset_index()

print("Якість даних після очищення:")
print(f"Загальна кількість записів: {len(df)}")
print(f"Пропущені значення: {df.isnull().sum().sum()}")
print(f"Дублікати: {df.duplicated().sum()}")
print(f"Викиди: {df['is_outlier'].sum()}")
```

### Документування процесу

**Метадані:**

-   Опис джерел даних
-   Схема даних (типи, обмеження)
-   Бізнес-правила та логіка
-   Частота оновлення

**Лог трансформацій:**

-   Список всіх застосованих змін
-   Обґрунтування рішень
-   Кількість впливових записів
-   Код для відтворення

**Валідаційний звіт:**

-   Статистики до/після очищення
-   Виявлені проблеми якості
-   Застосовані методи вирішення
-   Залишкові ризики

## Етап 3: Дослідження даних

**Розвідувальний аналіз даних (EDA)** — критично важливий етап для розуміння структури, патернів та аномалій у даних перед застосуванням складніших методів аналізу.

### Мета EDA

**Основні цілі:**

-   Розуміння структури та розподілу даних
-   Виявлення патернів та трендів
-   Пошук аномалій та викидів
-   Перевірка якості даних
-   Формування гіпотез для подальшого аналізу

### Описова статистика

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Базова інформація про датасет
print("Розмір датасету:", df.shape)
print("\nТипи даних:")
print(df.dtypes)

# Описова статистика для числових змінних
print("\nОписова статистика:")
print(df.describe())

# Інформація про категоріальні змінні
print("\nКатегоріальні змінні:")
for col in df.select_dtypes(include=['object']).columns:
    print(f"{col}: {df[col].nunique()} унікальних значень")
    print(df[col].value_counts().head())
    print("-" * 40)
```

### Візуалізація для розуміння

**1. Univariate аналіз (один змінна)**

```python
# Розподіл числових змінних
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Гістограма
df['amount'].hist(bins=50, ax=axes[0,0])
axes[0,0].set_title('Розподіл суми покупок')

# Box plot
df.boxplot(column='amount', ax=axes[0,1])
axes[0,1].set_title('Box plot суми покупок')

# Категоріальні змінні
df['category'].value_counts().plot(kind='bar', ax=axes[1,0])
axes[1,0].set_title('Розподіл по категоріях')

# Density plot
df['amount'].plot(kind='density', ax=axes[1,1])
axes[1,1].set_title('Щільність розподілу суми')

plt.tight_layout()
plt.show()
```

**2. Bivariate аналіз (дві змінні)**

```python
# Кореляційна матриця
correlation_matrix = df.select_dtypes(include=[np.number]).corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Кореляційна матриця')
plt.show()

# Scatter plots
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Залежність між змінними
sns.scatterplot(data=df, x='customer_age', y='amount', ax=axes[0])
axes[0].set_title('Вік клієнта vs Сума покупки')

# Групування по категоріях
sns.boxplot(data=df, x='category', y='amount', ax=axes[1])
axes[1].set_title('Сума покупки по категоріях')
axes[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
```

### Пошук патернів та аномалій

**1. Часові патерни**

```python
# Тренди в часі
df['month_year'] = df['purchase_date'].dt.to_period('M')
monthly_trends = df.groupby('month_year')['amount'].agg(['sum', 'count', 'mean'])

fig, axes = plt.subplots(3, 1, figsize=(15, 12))

# Загальна сума продажів
monthly_trends['sum'].plot(ax=axes[0], title='Загальний обсяг продажів по місяцях')

# Кількість транзакцій
monthly_trends['count'].plot(ax=axes[1], title='Кількість транзакцій по місяцях')

# Середній чек
monthly_trends['mean'].plot(ax=axes[2], title='Середній чек по місяцях')

plt.tight_layout()
plt.show()

# Сезонність
df['month'] = df['purchase_date'].dt.month
seasonal_pattern = df.groupby('month')['amount'].mean()
seasonal_pattern.plot(kind='bar', title='Сезонні патерни продажів')
plt.xlabel('Місяць')
plt.ylabel('Середня сума')
plt.show()
```

**2. Виявлення аномалій**

```python
# Статистичний метод (Z-score)
from scipy import stats

z_scores = np.abs(stats.zscore(df['amount']))
threshold = 3
outliers_zscore = df[z_scores > threshold]

print(f"Виявлено {len(outliers_zscore)} викидів методом Z-score")

# IQR метод
Q1 = df['amount'].quantile(0.25)
Q3 = df['amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers_iqr = df[(df['amount'] < lower_bound) | (df['amount'] > upper_bound)]
print(f"Виявлено {len(outliers_iqr)} викидів методом IQR")

# Візуалізація викидів
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.scatter(df.index, df['amount'], alpha=0.6)
plt.scatter(outliers_zscore.index, outliers_zscore['amount'], color='red', label='Z-score outliers')
plt.title('Викиди за Z-score методом')
plt.legend()

plt.subplot(1, 2, 2)
df.boxplot(column='amount')
plt.title('Box plot з викидами')

plt.tight_layout()
plt.show()
```

### Формування гіпотез

**На основі EDA формуємо гіпотези:**

1. **Гіпотеза про сезонність:**
   "Продажі значно вищі в грудні через новорічні свята"

2. **Гіпотеза про сегментацію:**
   "Клієнти віком 25-35 років роблять покупки на більші суми"

3. **Гіпотеза про категорії:**
   "Категорія 'Електроніка' має найвищу варіативність цін"

### Документування insights

```python
# Створення EDA звіту
eda_insights = {
    'dataset_size': df.shape,
    'missing_values': df.isnull().sum().to_dict(),
    'outliers_count': len(outliers_iqr),
    'top_categories': df['category'].value_counts().head(5).to_dict(),
    'avg_purchase_amount': df['amount'].mean(),
    'highest_sales_month': monthly_trends['sum'].idxmax(),
    'correlations': correlation_matrix.loc['amount'].sort_values(ascending=False).to_dict()
}

print("EDA Insights:")
for key, value in eda_insights.items():
    print(f"{key}: {value}")
```

## Етап 4: Моделювання та аналіз

На цьому етапі застосовуємо відповідні аналітичні методи для відповіді на поставлені бізнес-питання, базуючись на insights отриманих під час EDA.

### Вибір методів аналізу

**Критерії вибору методу:**

-   Тип бізнес-питання (описове, діагностичне, прогностичне)
-   Тип даних (числові, категоріальні, часові ряди)
-   Розмір вибірки
-   Припущення методу
-   Інтерпретованість результатів

### Статистичні тести

```python
from scipy import stats
import pandas as pd

# 1. Перевірка нормальності розподілу
# H0: Дані розподілені нормально
statistic, p_value = stats.shapiro(df['amount'])
print(f"Shapiro-Wilk test: p-value = {p_value:.4f}")
if p_value > 0.05:
    print("Дані розподілені нормально")
else:
    print("Дані НЕ розподілені нормально")

# 2. Порівняння груп (t-test)
# Порівняння середніх сум покупок між чоловіками та жінками
male_purchases = df[df['gender'] == 'Male']['amount']
female_purchases = df[df['gender'] == 'Female']['amount']

# Перевірка однорідності дисперсій
levene_stat, levene_p = stats.levene(male_purchases, female_purchases)
equal_var = levene_p > 0.05

# T-test
t_stat, t_p = stats.ttest_ind(male_purchases, female_purchases, equal_var=equal_var)
print(f"\nT-test порівняння покупок за статтю:")
print(f"t-statistic: {t_stat:.4f}, p-value: {t_p:.4f}")

# 3. ANOVA для порівняння декількох груп
from scipy.stats import f_oneway

groups = [group['amount'].values for name, group in df.groupby('category')]
f_stat, f_p = f_oneway(*groups)
print(f"\nANOVA по категоріях:")
print(f"F-statistic: {f_stat:.4f}, p-value: {f_p:.4f}")

# 4. Кореляційний аналіз
correlation_coef, corr_p = stats.pearsonr(df['customer_age'], df['amount'])
print(f"\nКореляція віку та суми покупки:")
print(f"Pearson r: {correlation_coef:.4f}, p-value: {corr_p:.4f}")
```

### Регресійний аналіз

```python
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm

# Підготовка даних для регресії
# Кодування категоріальних змінних
df_encoded = pd.get_dummies(df, columns=['category', 'gender'])

# Вибір змінних
features = ['customer_age'] + [col for col in df_encoded.columns if col.startswith(('category_', 'gender_'))]
X = df_encoded[features]
y = df_encoded['amount']

# Розділення на train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. Scikit-learn підхід
model = LinearRegression()
model.fit(X_train, y_train)

# Прогнозування
y_pred = model.predict(X_test)

# Оцінка якості
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"R²: {r2:.4f}")

# 2. Statsmodels для детальної статистики
X_train_sm = sm.add_constant(X_train)  # Додаємо константу
model_sm = sm.OLS(y_train, X_train_sm).fit()
print("\nДетальна статистика моделі:")
print(model_sm.summary())
```

### Машинне навчання

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score, GridSearchCV

# Random Forest модель
rf_model = RandomForestRegressor(random_state=42)

# Grid Search для оптимізації гіперпараметрів
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

# Найкращі параметри
print("Найкращі параметри:", grid_search.best_params_)
print("Найкращий R² score:", grid_search.best_score_)

# Важливість змінних
best_rf = grid_search.best_estimator_
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': best_rf.feature_importances_
}).sort_values('importance', ascending=False)

print("\nВажливість змінних:")
print(feature_importance)
```

### Валідація результатів

```python
# Cross-validation для оцінки стабільності моделі
cv_scores = cross_val_score(best_rf, X, y, cv=10, scoring='r2')
print(f"\nCross-validation R² scores: {cv_scores}")
print(f"Середній R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# Перевірка залишків
residuals = y_test - y_pred

# Візуалізація залишків
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Q-Q plot для перевірки нормальності залишків
from scipy.stats import probplot
probplot(residuals, dist="norm", plot=axes[0])
axes[0].set_title("Q-Q Plot залишків")

# Графік залишків vs прогнозовані значення
axes[1].scatter(y_pred, residuals, alpha=0.6)
axes[1].axhline(y=0, color='red', linestyle='--')
axes[1].set_xlabel('Прогнозовані значення')
axes[1].set_ylabel('Залишки')
axes[1].set_title('Залишки vs Прогнози')

plt.tight_layout()
plt.show()

# Тестування припущень
print("\nТестування припущень:")
print(f"Автокореляція залишків (Durbin-Watson): {sm.stats.stattools.durbin_watson(residuals):.4f}")
```

## Етап 5: Інтерпретація результатів

Найвідповідальніший етап — правильна інтерпретація отриманих результатів та їх перекладення на бізнес-мову.

### Статистична значущість

```python
# Інтерпретація p-values та довірчих інтервалів
alpha = 0.05  # Рівень значущості

print("Інтерпретація результатів:")
print(f"Рівень значущості: {alpha}")

# Для t-test
if t_p < alpha:
    print(f"✓ Існує статистично значуща різниця між групами (p = {t_p:.4f})")
    print(f"  Середня різниця: {male_purchases.mean() - female_purchases.mean():.2f}")
else:
    print(f"✗ Немає статистично значущої різниці між групами (p = {t_p:.4f})")

# Для регресії
significant_features = model_sm.pvalues[model_sm.pvalues < alpha]
print(f"\nСтатистично значущі змінні:")
for feature, p_val in significant_features.items():
    coef = model_sm.params[feature]
    print(f"  {feature}: коефіцієнт = {coef:.4f}, p-value = {p_val:.4f}")
```

### Бізнес-інтерпретація

```python
# Переклад статистичних результатів на бізнес-мову
business_insights = []

# 1. Аналіз коефіцієнтів регресії
for feature, coef in zip(features, model.coef_):
    if abs(coef) > 10:  # Поріг практичної значущості
        if 'age' in feature:
            business_insights.append(f"Збільшення віку клієнта на 1 рік в середньому збільшує суму покупки на ${coef:.2f}")
        elif 'category_' in feature:
            category_name = feature.replace('category_', '')
            if coef > 0:
                business_insights.append(f"Клієнти категорії '{category_name}' витрачають в середньому на ${coef:.2f} більше")

# 2. Практична значущість
effect_size = (male_purchases.mean() - female_purchases.mean()) / df['amount'].std()
if abs(effect_size) > 0.2:  # Cohen's d > 0.2 - малий ефект
    business_insights.append(f"Практично значуща різниця між чоловіками та жінками (effect size = {effect_size:.2f})")

print("Бізнес-insights:")
for i, insight in enumerate(business_insights, 1):
    print(f"{i}. {insight}")
```

### Обмеження та припущення

```python
# Документування обмежень аналізу
limitations = [
    f"Аналіз базується на {len(df)} спостереженнях за період {df['purchase_date'].min()} - {df['purchase_date'].max()}",
    f"R² моделі = {r2:.3f}, тобто модель пояснює {r2*100:.1f}% варіації",
    f"Виключено {len(outliers_iqr)} викидів ({len(outliers_iqr)/len(df)*100:.1f}% від загальної вибірки)",
    "Припущення лінійної залежності може не відповідати реальності",
    "Результати можуть не поширюватися на інші періоди через сезонність"
]

print("Обмеження аналізу:")
for i, limitation in enumerate(limitations, 1):
    print(f"{i}. {limitation}")

# Рекомендації для покращення аналізу
recommendations = [
    "Збільшити вибірку для підвищення статистичної потужності",
    "Додати змінні про маркетингові активності",
    "Розглянути нелінійні моделі",
    "Провести аналіз для різних сезонів окремо"
]

print("\nРекомендації для майбутніх досліджень:")
for i, rec in enumerate(recommendations, 1):
    print(f"{i}. {rec}")
```

## Етап 6: Комунікація та впровадження

<!-- TODO: Презентація результатів -->
<!-- Візуалізація для різних аудиторій -->
<!-- Рекомендації та план дій -->
<!-- Моніторинг впровадження -->

## Ітеративність процесу

<!-- TODO: Пояснення циклічності -->
<!-- Continuous improvement -->
<!-- Feedback loops -->
<!-- Agile підходи в аналітиці -->

---

**Навігація:**

-   ⬅️ [Роль дата-аналітика в сучасному світі](./02_роль_дата_аналітика.md)
-   ➡️ [Описова статистика](./04_описова_статистика.md)

_Останнє оновлення: 04.08.2025_
